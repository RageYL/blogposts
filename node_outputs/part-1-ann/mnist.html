
	<!DOCTYPE html>
	<html>
	<head>
		<meta charset="UTF-8">

		<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/normalize/7.0.0/normalize.min.css" />

		<link href="https://fonts.googleapis.com/css?family=Nunito" rel="stylesheet">

		<!--
		<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/meyer-reset/2.0/reset.min.css" />
		-->

		<link rel="stylesheet" href="/node_sources/part-x/index-v1.css">
		<link rel="stylesheet" href="/node_modules/katex/dist/katex.min.css">
		<link rel="stylesheet" href="/node_modules/prismjs/themes/prism-okaidia.css">

		<link rel="stylesheet" href="/node_modules/slick-carousel/slick/slick.css">
		<link rel="stylesheet" href="/node_modules/slick-carousel/slick/slick-theme.css">

		<script src="/node_modules/lodash/lodash.min.js"></script>
		<script src="/node_modules/highcharts/highcharts.js"></script>
		<script src="/node_modules/jquery/dist/jquery.min.js"></script>
		<script src="/node_modules/slick-carousel/slick/slick.min.js"></script>

		<link rel="stylesheet" href="//code.jquery.com/ui/1.12.1/themes/base/jquery-ui.css">
		<script src="https://code.jquery.com/ui/1.12.1/jquery-ui.js"></script>
	</head>
	<body>
		<article>
			<header>
				<h1>Welcome to Machine Learning<h1>
				<h1>Part 1 - Neural Networks</h1>
			</header>
			<div id="article-content">
				<h1 id="mnist">Mnist</h1>
<p>Until now we always trained our network will all the inputs it will ever need to
make a prediction for. Thus we had 100% correct predictions. But in the real
world, this is not something feasable. For our digits recognition task, there
exist an infinity of possible images. What's matter is to train our network with
a variety of different images so that it will still be able to make prediction
for images it has never seen.</p>
<p>So our first task is to get a dataset of labelled images. There exists plenty of
free datasets available online and some of the most famous are directly
integrated into keras. One of them, the mnist dataset, contains labelled
pictures of hand written digits.</p>
<pre class="language-python"><code class="language-python"><span class="token keyword">from</span> keras<span class="token punctuation">.</span>datasets <span class="token keyword">import</span> mnist
</code></pre>
<p>We then load the inputs and labels. They are splits in two datasets:</p>
<ul>
<li>train: the one we use to train our network</li>
<li>test : the one we use to see how well our network perform with new data</li>
</ul>
<pre class="language-python"><code class="language-python"><span class="token punctuation">(</span>x_train<span class="token punctuation">,</span> y_train<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>x_test<span class="token punctuation">,</span> y_test<span class="token punctuation">)</span> <span class="token operator">=</span> mnist<span class="token punctuation">.</span>load_data<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre>
<p>Let's play with our input data:</p>
<pre class="language-python"><code class="language-python">x_train<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>shape

<span class="token comment">#> (28, 28)</span>
</code></pre>
<p>We can see that our images are represented by 2d arrays. Let's look at the first
one:</p>
<pre class="language-python"><code class="language-python">x_train<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
<span class="token comment">#> array([[  0, 0, 0, 0, 0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 0, 0, 0],</span>
<span class="token comment">#>        [  0, 0, 0, 0, 0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 0, 0, 0],</span>
<span class="token comment">#>        [  0, 0, 0, 0, 0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 0, 0, 0],</span>
<span class="token comment">#>        [  0, 0, 0, 0, 0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 0, 0, 0],</span>
<span class="token comment">#>        [  0, 0, 0, 0, 0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 0, 0, 0],</span>
<span class="token comment">#>        [  0, 0, 0, 0, 0,   0,   0,   0,   0,   0,   0,   0,   3,   18,  18,  18,  126, 136, 175, 26,  166, 255, 247, 127, 0, 0, 0, 0],</span>
<span class="token comment">#>        [  0, 0, 0, 0, 0,   0,   0,   0,   30,  36,  94,  154, 170, 253, 253, 253, 253, 253, 225, 172, 253, 242, 195, 64,  0, 0, 0, 0],</span>
<span class="token comment">#>        [  0, 0, 0, 0, 0,   0,   0,   49,  238, 253, 253, 253, 253, 253, 253, 253, 253, 251, 93,  82,  82,  56,  39,  0,   0, 0, 0, 0],</span>
<span class="token comment">#>        [  0, 0, 0, 0, 0,   0,   0,   18,  219, 253, 253, 253, 253, 253, 198, 182, 247, 241, 0,   0,   0,   0,   0,   0,   0, 0, 0, 0],</span>
<span class="token comment">#>        [  0, 0, 0, 0, 0,   0,   0,   0,   80,  156, 107, 253, 253, 205, 11,  0,   43,  154, 0,   0,   0,   0,   0,   0,   0, 0, 0, 0],</span>
<span class="token comment">#>        [  0, 0, 0, 0, 0,   0,   0,   0,   0,   14,  1,   154, 253, 90,  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 0, 0, 0],</span>
<span class="token comment">#>        [  0, 0, 0, 0, 0,   0,   0,   0,   0,   0,   0,   139, 253, 190, 2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 0, 0, 0],</span>
<span class="token comment">#>        [  0, 0, 0, 0, 0,   0,   0,   0,   0,   0,   0,   11,  190, 253, 70,  0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 0, 0, 0],</span>
<span class="token comment">#>        [  0, 0, 0, 0, 0,   0,   0,   0,   0,   0,   0,   0,   35,  241, 225, 160, 108, 1,   0,   0,   0,   0,   0,   0,   0, 0, 0, 0],</span>
<span class="token comment">#>        [  0, 0, 0, 0, 0,   0,   0,   0,   0,   0,   0,   0,   0,   81,  240, 253, 253, 119, 25,  0,   0,   0,   0,   0,   0, 0, 0, 0],</span>
<span class="token comment">#>        [  0, 0, 0, 0, 0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   45,  186, 253, 253, 150, 27,  0,   0,   0,   0,   0, 0, 0, 0],</span>
<span class="token comment">#>        [  0, 0, 0, 0, 0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   16,  93,  252, 253, 187, 0,   0,   0,   0,   0, 0, 0, 0],</span>
<span class="token comment">#>        [  0, 0, 0, 0, 0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   249, 253, 249, 64,  0,   0,   0,   0, 0, 0, 0],</span>
<span class="token comment">#>        [  0, 0, 0, 0, 0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   46,  130, 183, 253, 253, 207, 2,   0,   0,   0,   0, 0, 0, 0],</span>
<span class="token comment">#>        [  0, 0, 0, 0, 0,   0,   0,   0,   0,   0,   0,   0,   39,  148, 229, 253, 253, 253, 250, 182, 0,   0,   0,   0,   0, 0, 0, 0],</span>
<span class="token comment">#>        [  0, 0, 0, 0, 0,   0,   0,   0,   0,   0,   24,  114, 221, 253, 253, 253, 253, 201, 78,  0,   0,   0,   0,   0,   0, 0, 0, 0],</span>
<span class="token comment">#>        [  0, 0, 0, 0, 0,   0,   0,   0,   23,  66,  213, 253, 253, 253, 253, 198, 81,  2,   0,   0,   0,   0,   0,   0,   0, 0, 0, 0],</span>
<span class="token comment">#>        [  0, 0, 0, 0, 0,   0,   18,  171, 219, 253, 253, 253, 253, 195, 80,  9,   0,   0,   0,   0,   0,   0,   0,   0,   0, 0, 0, 0],</span>
<span class="token comment">#>        [  0, 0, 0, 0, 55,  172, 226, 253, 253, 253, 253, 244, 133, 11,  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 0, 0, 0],</span>
<span class="token comment">#>        [  0, 0, 0, 0, 136, 253, 253, 253, 212, 135, 132, 16,  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 0, 0, 0],</span>
<span class="token comment">#>        [  0, 0, 0, 0, 0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 0, 0, 0],</span>
<span class="token comment">#>        [  0, 0, 0, 0, 0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 0, 0, 0],</span>
<span class="token comment">#>        [  0, 0, 0, 0, 0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 0, 0, 0]], dtype=uint8)</span>
</code></pre>
<p>While we were using only black and white pixels, here we have greyscale images.
So pixels value range from 0 to 255, 0 being black, 255 being white and all the
values in between being shades of grey.</p>
<p>We can guess from the pattern that the image represent a 5, but let's display
the image to confirm:</p>
<pre class="language-python"><code class="language-python">imshow<span class="token punctuation">(</span>x_train<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

<span class="token comment">#> insert in notebook &amp; blog ?</span>
</code></pre>
<p>Looks like our prediction was right. But we could also have used the labels
associated with this image:</p>
<pre class="language-python"><code class="language-python">y_train<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>

<span class="token comment">#> 5</span>
</code></pre>
<p>So we have inputs and we have labels. Let's build our network !</p>
<p>Currently we only learned how to create network with 1d inputs, so we need to
reshape the input from 28x28 to an array of 784 elements:</p>
<pre class="language-python"><code class="language-python">x_test  <span class="token operator">=</span> x_test <span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token punctuation">(</span>x_test <span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
x_train <span class="token operator">=</span> x_train<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token punctuation">(</span>x_train<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

x_train<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>shape

<span class="token comment">#> (784,)</span>
</code></pre>
<p>Now we can pass those array as inputs.</p>
<p>Let's begin with a simple network with a single hidden layer.
We said that hidden layers should have an activation and that ReLu was a good
default. We also said that Adam was a good optimizer, so let's use them.
As for the output, we have 10 classes, one for each digit, from 0 to 9. And we
want to predict which class is the most likely. So we will use 10 neurons, with
a softmax activation and a categorical_crossentropy loss.
But we've seens that the labels are represented by numbers and we know that the
network only accept one hot vectors. So we ask keras to convert those numbers to
one hot vectors by using sparse_categorical_crossentropy.</p>
<p>Now let's pick an arbitrary number of neurons and train the network for a few
steps.</p>
<p>TODO: image of current network</p>
<pre class="language-python"><code class="language-python"><span class="token keyword">from</span> keras<span class="token punctuation">.</span>models <span class="token keyword">import</span> Sequential
<span class="token keyword">from</span> keras<span class="token punctuation">.</span>layers <span class="token keyword">import</span> Dense

model <span class="token operator">=</span> Sequential<span class="token punctuation">(</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Dense<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span> activation <span class="token operator">=</span> <span class="token string">"relu"</span><span class="token punctuation">,</span> input_shape <span class="token operator">=</span> x_train<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Dense<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> activation <span class="token operator">=</span> <span class="token string">"softmax"</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

model<span class="token punctuation">.</span><span class="token builtin">compile</span><span class="token punctuation">(</span>optimizer <span class="token operator">=</span> <span class="token string">"adam"</span><span class="token punctuation">,</span> loss <span class="token operator">=</span> <span class="token string">"sparse_categorical_crossentropy"</span><span class="token punctuation">,</span> metrics <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">"accuracy"</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>x_train<span class="token punctuation">,</span> y_train<span class="token punctuation">,</span> epochs <span class="token operator">=</span> <span class="token number">3</span><span class="token punctuation">)</span>
</code></pre>
<p>TODO: image predict &amp; plot</p>
<p>Run the last line until the accuracy stall. It stopped close to 0.6 on my
computer (don't worry if you don't find the same results and keep going). That
means that out of 10 pictures, 6 are correctly classified. Not bad for 5 lines
of code and a few seconds of computation. But this accuracy is not really useful
because the network has been trained on those images. It would be better to see
how it performs on unseen images. Let's use the test dataset we kept for this
exact reason.</p>
<pre class="language-python"><code class="language-python">model<span class="token punctuation">.</span>evaluate<span class="token punctuation">(</span>x_test<span class="token punctuation">,</span> y_test<span class="token punctuation">)</span>

<span class="token comment">#></span>
</code></pre>
<p>The first number is the value of the loss and the second is the accuracy. You
should get about the same accuracy you got on the training dataset. Seems like
our network perform the same on both datasets.</p>
<p>As we said, we have to try some different architectures and keep the one that
work best. So let's try with more neurons:</p>
<pre class="language-python"><code class="language-python">model <span class="token operator">=</span> Sequential<span class="token punctuation">(</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Dense<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">,</span> activation <span class="token operator">=</span> <span class="token string">"relu"</span><span class="token punctuation">,</span> input_shape <span class="token operator">=</span> x_train<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Dense<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> activation <span class="token operator">=</span> <span class="token string">"softmax"</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

model<span class="token punctuation">.</span><span class="token builtin">compile</span><span class="token punctuation">(</span>optimizer <span class="token operator">=</span> <span class="token string">"adam"</span><span class="token punctuation">,</span> loss <span class="token operator">=</span> <span class="token string">"sparse_categorical_crossentropy"</span><span class="token punctuation">,</span> metrics <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">"accuracy"</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>x_train<span class="token punctuation">,</span> y_train<span class="token punctuation">,</span> epochs <span class="token operator">=</span> <span class="token number">3</span><span class="token punctuation">)</span>

model<span class="token punctuation">.</span>evaluate<span class="token punctuation">(</span>x_test<span class="token punctuation">,</span> y_test<span class="token punctuation">)</span>
</code></pre>
<p>After running for 6 epochs, I got a better score of 0.7 for both datasets: 7
images out of 10 are correctly classified.</p>
<p>Let's try with a deeper network:</p>
<pre class="language-python"><code class="language-python">model <span class="token operator">=</span> Sequential<span class="token punctuation">(</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Dense<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span> activation <span class="token operator">=</span> <span class="token string">"relu"</span><span class="token punctuation">,</span> input_shape <span class="token operator">=</span> x_train<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Dense<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span> activation <span class="token operator">=</span> <span class="token string">"relu"</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Dense<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span> activation <span class="token operator">=</span> <span class="token string">"relu"</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Dense<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> activation <span class="token operator">=</span> <span class="token string">"softmax"</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

model<span class="token punctuation">.</span><span class="token builtin">compile</span><span class="token punctuation">(</span>optimizer <span class="token operator">=</span> <span class="token string">"adam"</span><span class="token punctuation">,</span> loss <span class="token operator">=</span> <span class="token string">"sparse_categorical_crossentropy"</span><span class="token punctuation">,</span> metrics <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">"accuracy"</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>x_train<span class="token punctuation">,</span> y_train<span class="token punctuation">,</span> epochs <span class="token operator">=</span> <span class="token number">6</span><span class="token punctuation">)</span>

model<span class="token punctuation">.</span>evaluate<span class="token punctuation">(</span>x_test<span class="token punctuation">,</span> y_test<span class="token punctuation">)</span>
</code></pre>
<p>Now after 6 epochs, I get an accuracy greater than 0.9 on both datasets. Do not
forget to evaluate your network on your test dataset otherwise you could end up
predicting the good value 100% of the time for images you've trained on but
perform poorly on unseen images. This is called overfitting.</p>
<p>Now let's say that this accuracy suits us, we can use this network to make
predictions with <code>predict_classes()</code>:</p>
<pre class="language-python"><code class="language-python">model<span class="token punctuation">.</span>predict_classes<span class="token punctuation">(</span>x_test<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">10</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
</code></pre>
<p>If we spent more time playing with the network we could get an even better
accuracy but it's not our objective here.</p>
<p>Now let's take a step back: why did we do all this ? Couldn't we come up with a
program that perform better and gives us the the good answer each time ? Well...
no. Recognizing a digit may seems a simple task, but imagine all the positions
and deformations a digit can undergo. There would be too many cases your program
would fail to handle. Instead we let the networks figure out those rules for us.
And it does a better job at finding those rules than programs built by humans
experts.
It's also worth noting that the rules the network learn are often different from
what a human would come with. Thus we often don't understand what our networks
are doing.</p>
<p>TODO: include example building block neurons
TODO: talk about feature engineering</p>
<p>I'd would also like to point out that we may think that a human would have 100%
accuracy on this kind of task, but that's not true. Take thoses pictures for
example:</p>
<p>TODO: images</p>
<p>It was ''. many people would get them wrong ! So there is always some small
margin for computers to improve upon human. And in fact for single character
recognition, machines are already better than humans since a few years =)</p>
<p>And this is the end of our first machine learning project.</p>
<hr>
<p>This post has been long, and there is so much things left to cover.</p>
<p>TODO: ... (move to another file)</p>
<p>We also only looked at the bright side of neural networks. However they have
numerous pitfalls that we didn't talk about.</p>
<p>I hope this post helped you !</p>

			</div>
		</article>
	</body>
	</html>
	