
	<!DOCTYPE html>
	<html>
	<head>
		<meta charset="UTF-8">

		<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/normalize/7.0.0/normalize.min.css" />

		<link href="https://fonts.googleapis.com/css?family=Nunito" rel="stylesheet">

		<!--
		<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/meyer-reset/2.0/reset.min.css" />
		-->

		<link rel="stylesheet" href="/node_sources/part-x/index-v1.css">
		<link rel="stylesheet" href="/node_modules/katex/dist/katex.min.css">
		<link rel="stylesheet" href="/node_modules/prismjs/themes/prism-okaidia.css">

		<link rel="stylesheet" href="/node_modules/slick-carousel/slick/slick.css">
		<link rel="stylesheet" href="/node_modules/slick-carousel/slick/slick-theme.css">

		<script src="/node_modules/lodash/lodash.min.js"></script>
		<script src="/node_modules/highcharts/highcharts.js"></script>
		<script src="/node_modules/jquery/dist/jquery.min.js"></script>
		<script src="/node_modules/slick-carousel/slick/slick.min.js"></script>

		<link rel="stylesheet" href="//code.jquery.com/ui/1.12.1/themes/base/jquery-ui.css">
		<script src="https://code.jquery.com/ui/1.12.1/jquery-ui.js"></script>
	</head>
	<body>
		<article>
			<header>
				<h1>Welcome to Machine Learning<h1>
				<h1>Part 1 - Neural Networks</h1>
			</header>
			<div id="article-content">
				<h1 id="training">Training</h1>
<p>With what we learned we can explain most of the <code>train</code> function. First let's
take a look at:</p>
<h2 id="compile">Compile</h2>
<pre class="language-python"><code class="language-python">model<span class="token punctuation">.</span><span class="token builtin">compile</span><span class="token punctuation">(</span>optimizer <span class="token operator">=</span> <span class="token string">"sgd"</span><span class="token punctuation">,</span> loss <span class="token operator">=</span> <span class="token string">"mse"</span><span class="token punctuation">,</span> metrics <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">"accuracy"</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
</code></pre>
<p>This line configure our model for training</p>
<h2 id="loss">Loss</h2>
<p>As we saw earlier, we need a function to evaluate how well our network perform.
Those functions are called loss or cost functions.</p>
<p>Here we used MSE, which is the short for Mean Squared Error. Its the one we
described and used in our example.</p>
<p>Like activations, there is a few well known losses that we will cover in the
next section. Note that unlike activations it's not rare to create your own loss.</p>
<h2 id="optimizer">Optimizer</h2>
<p>Optimizers are the functions used to update the weights. In our example we
implemented Stochastic Gradient Descent or SGD for short.  There is several
other optimizers that all use the derivatives but update the weight in a
different way to improve speed and accuracy. Nowadays a good default is adam.</p>
<h2 id="metrics">Metrics</h2>
<p>Metrics is just an array that specify what information should be displayed while
training. &quot;accuracy&quot; represent the percentage of correctly classified input</p>
<ul>
<li>1.0000: 100% of the inputs have been correctly classified</li>
<li>0.7500:  75% of the inputs have been correctly classified</li>
</ul>
<pre class="language-python"><code class="language-python"><span class="token comment">#> 2/2 [==============================] - 0s - loss: 1.0960e-07 - acc: 1.0000</span>
</code></pre>
<p>Here we can see that 'acc' is displayed. By default only the loss is displayed.</p>
<p>Until now the dataset represented all the value we wanted to predict, but in
real applications, we only have some examples. the objective is to have enought
different examples so that the function we learn will predict good values for
unseen inputs.</p>
<p>TODO: graph</p>
<h2 id="fit">Fit</h2>
<pre class="language-python"><code class="language-python">model<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>inputs<span class="token punctuation">,</span> labels<span class="token punctuation">,</span> epochs <span class="token operator">=</span> <span class="token number">2000</span><span class="token punctuation">)</span>
</code></pre>
<p>This line actually train our model.</p>
<p>We are already familiar with inputs and labels, those are the pairs used to
train our network.</p>
<p>Epochs is the number of steps we want to take, i.e the number of times we
improve our weights.</p>
<p>You should now be able to understand the whole output:</p>
<pre class="language-python"><code class="language-python"><span class="token comment">#> Epoch 2000/2000</span>
<span class="token comment">#> 2/2 [==============================] - 0s - loss: 1.0960e-07 - acc: 1.0000</span>
</code></pre>
<p>You now understand our whole code !</p>
<h1 id="output-layers">Output Layers</h1>
<p>Until now we only predicted a single number. But neural networks can be used to predict
much more complex data. Let's see how we can do that !</p>
<h2 id="regression">Regression</h2>
<p>The ouput layer is used to give you the information you want. Here is a network
with a single output:</p>
<p><img src="/node_outputs/part-1-ann/images-js/nnet-3331.png" alt=""></p>
<p>The most simple thing that an output neuron can return is a number. Let's see a
few example:</p>
<ul>
<li>given the size of a house, its number of rooms, predict its price.</li>
<li>given an image, predict the x position of an object</li>
<li>given an image, predict the y position of an object</li>
</ul>
<p>Instead of having two networks, one predicting the x coordinate and a second
predicting the y coordinate, you can use two neurons on the output layer and
create a single network that predict the x and y value at once:</p>
<p><img src="/node_outputs/part-1-ann/images-js/nnet-3332.png" alt=""></p>
<p>The kind of tasks where you try to predict a continous value (like when we
inverted the sign of the input number), represent regression problems. For these
problems use one neuron per value you want to predict and use a mean squared
error loss.</p>
<p>When having multiple outputs with keras instead of having a label represented by
a single value, we use an array:</p>
<pre class="language-python"><code class="language-python"><span class="token comment"># one output</span>
labels <span class="token operator">=</span> <span class="token punctuation">[</span>
    <span class="token number">10</span><span class="token punctuation">,</span> <span class="token comment"># label for input 1</span>
    <span class="token number">20</span><span class="token punctuation">,</span> <span class="token comment"># label for input 2</span>
    <span class="token number">30</span><span class="token punctuation">,</span> <span class="token comment"># label for input 3</span>
<span class="token punctuation">]</span>

<span class="token comment"># two outputs</span>
labels <span class="token operator">=</span> <span class="token punctuation">[</span>
    <span class="token punctuation">[</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">40</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token comment"># label for input 1</span>
    <span class="token punctuation">[</span><span class="token number">20</span><span class="token punctuation">,</span> <span class="token number">50</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token comment"># label for input 2</span>
    <span class="token punctuation">[</span><span class="token number">30</span><span class="token punctuation">,</span> <span class="token number">60</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token comment"># label for input 3</span>
<span class="token punctuation">]</span>
</code></pre>
<h2 id="classification">Classification</h2>
<h3 id="binary-classification">Binary Classification</h3>
<p>With one neuron we can also do binary classification: that's what we did until
now. Binary because we classify the input between two categories/classes:</p>
<ul>
<li>given a scan, predict if it's cancer or not cancer.</li>
<li>given an email, predict if it's a spam or not a spam.</li>
</ul>
<p>In these tasks, the return value is called the confidence. This is because you
often want to return 1 when you are 100% sure that the input belongs to the
current class and 0 when you are 100% sure that the input do not belongs to the
class. But you can also return intermediate value to express how much confident
you are in your response. In a spam system:</p>
<ul>
<li>confidence = 0.20 (20%): the mail is unlikely to be a spam (80% it is not)</li>
<li>confidence = 0.50 (50%): we don't know whether this is a spam or not</li>
<li>confidence = 0.90 (90%): the mail is almost certainly a spam</li>
</ul>
<p>To enforce a value between 0 and 1 (having a confidence of 110% is not very
helpful) we often use a sigmoid activation. As for the loss we can use
<code>binary_crossentropy</code>. We don't need to understand what is does, we just need to
know that it's a loss that works particularly well for this kind of problems.</p>
<p>Like for regression, if your input can belongs to multiple classes, you just have
to add more neurons on the output layer.</p>
<p>TODO: image</p>
<div class="note">
<p>In this tutorial we used <code>mean_squared_error</code> all along, but we should have been
using <code>binary_crossentropy</code> because we were doing binary classification.
Unfortunately it would have required bigger networks from the start and I wanted
to avoid that.</p>
</div>
<h3 id="multiclass-classification">Multiclass Classification</h3>
<p>We've just covered binary classification, but in fact it's just a special case
of classification. Instead of using one neuron we could have represented our
problem with two neurons:</p>
<ul>
<li>one that predicts the confidence of being a spam</li>
<li>one that predicts the confidence of not being a spam</li>
</ul>
<p>TODO: image</p>
<p>In that case, we should have used another loss: <code>categorical_crossentropy</code>.</p>
<p>But note that there is something else we need to take care of. With only one
neuron, if we had 80% confidence in the mail being a spam, we could easely
compute the confidence of the mail not being a spam with <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>1</mn><mi mathvariant="normal">.</mi><mn>0</mn><mn>0</mn><mo>−</mo><mn>0</mn><mi mathvariant="normal">.</mi><mn>8</mn><mn>0</mn><mo>=</mo><mn>0</mn><mi mathvariant="normal">.</mi><mn>2</mn><mn>0</mn></mrow><annotation encoding="application/x-tex">1.00 - 0.80 = 0.20</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.64444em;"></span><span class="strut bottom" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="base textstyle uncramped"><span class="mord mathrm">1</span><span class="mord mathrm">.</span><span class="mord mathrm">0</span><span class="mord mathrm">0</span><span class="mbin">−</span><span class="mord mathrm">0</span><span class="mord mathrm">.</span><span class="mord mathrm">8</span><span class="mord mathrm">0</span><span class="mrel">=</span><span class="mord mathrm">0</span><span class="mord mathrm">.</span><span class="mord mathrm">2</span><span class="mord mathrm">0</span></span></span></span>.</p>
<p>Now each neuron can output a value between 0 and 1. We could end up with both of
them outputing a 80% confidence. Being 80% sure the mail is a spam and 80% sure
the mail is not a spam is just meaningless. We need a way to ensure that both
probabilities sum to 1.</p>
<p>And once again there is a simple solution: using a softmax activation. Let's
take an example to see how it works. Each neuron can output any number: (TODO:
images)</p>
<p>neuron 1 = 30<br>
neuron 2 = 90</p>
<p>Those values are then passed to the softmax layer. A bigger value should have a
bigger confidence and a lower value should have a lower confidence. We begin by
summing all the values:</p>
<p><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>3</mn><mn>0</mn><mo>+</mo><mn>9</mn><mn>0</mn><mo>=</mo><mn>1</mn><mn>2</mn><mn>0</mn></mrow><annotation encoding="application/x-tex">30 + 90 = 120</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.64444em;"></span><span class="strut bottom" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="base textstyle uncramped"><span class="mord mathrm">3</span><span class="mord mathrm">0</span><span class="mbin">+</span><span class="mord mathrm">9</span><span class="mord mathrm">0</span><span class="mrel">=</span><span class="mord mathrm">1</span><span class="mord mathrm">2</span><span class="mord mathrm">0</span></span></span></span></p>
<p>Then for each neuron we want to know what percentage of this sum it represent:</p>
<p><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>3</mn><mn>0</mn><mi mathvariant="normal">/</mi><mn>1</mn><mn>2</mn><mn>0</mn><mo>=</mo><mn>0</mn><mi mathvariant="normal">.</mi><mn>2</mn><mn>5</mn></mrow><annotation encoding="application/x-tex">30 / 120 = 0.25</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord mathrm">3</span><span class="mord mathrm">0</span><span class="mord mathrm">/</span><span class="mord mathrm">1</span><span class="mord mathrm">2</span><span class="mord mathrm">0</span><span class="mrel">=</span><span class="mord mathrm">0</span><span class="mord mathrm">.</span><span class="mord mathrm">2</span><span class="mord mathrm">5</span></span></span></span><br>
<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>9</mn><mn>0</mn><mi mathvariant="normal">/</mi><mn>1</mn><mn>2</mn><mn>0</mn><mo>=</mo><mn>0</mn><mi mathvariant="normal">.</mi><mn>7</mn><mn>5</mn></mrow><annotation encoding="application/x-tex">90 / 120 = 0.75</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord mathrm">9</span><span class="mord mathrm">0</span><span class="mord mathrm">/</span><span class="mord mathrm">1</span><span class="mord mathrm">2</span><span class="mord mathrm">0</span><span class="mrel">=</span><span class="mord mathrm">0</span><span class="mord mathrm">.</span><span class="mord mathrm">7</span><span class="mord mathrm">5</span></span></span></span></p>
<p>Now we are sure each neuron output a percentage between 0 and 1 and that they
all sum to 1.</p>
<p>For those interested softmax add just a little trick to the above: it replace
all values with an exponential so that bigger values get even bigger confidences
and lower values get even lower confidences.</p>
<p>TODO: softmax equation when frac will works...</p>
<p>So why did we bother doing all those changes while we could have done the same
thing with our previous one neuron method ? Well, now we can add as many classes
as we want and it will predict which of these classes is the most probable.</p>
<p>TODO: image</p>
<p>It should be useful for our project ;)</p>
<p>For this kind of problem, the labels are passed as one hot vectors. One hot
vectors are arrays filled with 0 except for one value that will be 1. Let's say
we want to detect what animal is represented in an image. We have 3 possible
classes: bird, cat and dog.</p>
<ul>
<li>If the image represent a bird, the one hot vector will be [1, 0, 0]</li>
<li>If the image represent a cat,  the one hot vector will be [0, 1, 0]</li>
<li>If the image represent a dog,  the one hot vector will be [0, 0, 1]</li>
</ul>
<p>If you have thousands of classes, it can become annoying to create those array.
Instead we would prefer using a number to represent the class represented in the
image.</p>
<ul>
<li>If the image represent a bird, the label is 0</li>
<li>If the image represent a cat,  the label is 1</li>
<li>If the image represent a dog,  the label is 2</li>
</ul>
<p>And fortunately keras allows us to specify labels as number and will convert
them as one hot vectors by using <code>sparse_categorical_crossentropy</code>.</p>
<h2 id="classification-example-keep-or-not-simplify">Classification Example (Keep or Not ? Simplify ?)</h2>
<p>We've seen how to solve many different problems and one nice thing is that we
can mix those methods. Let's build a network for the following task: we have an
image and we want to know if there is an animal in it.
It's a classic binary classification problem, so we can use one neuron with a
sigmoid activation and a binary_crossentropy loss. If the output is &gt; 0.5 then
there is an animal otherwise, there is not.
Now if there is an animal we also want to predict its position in the image.
So we keep the previous network and we add two new neuron, one for predicting x
and one for predicting y. Both can return any value so we don't use any
activation and we use a mean squared error loss as this is a regression problem.
Then we also want to know which kind of animal it is.
So once again we reuse the previous network and add more neurons: one per kind
of animal. These neurons will then be followed by a softmax activation layer and
will use a categorical_crossentropy loss.</p>
<p>And that's it, we built a network that can perform all those tasks from small
buildings blocks. As long you have the desired labels, you can train your
network to perform these tasks.</p>
<p>While building these more complex networks is pretty straightforward with
libraries like keras, it would require us to learn a few more things about
keras' api that i don't want to cover in this post. But take a look at keras'
functional API if you want to know.</p>
<div class="note">
<p>One may think that by giving many tasks to a single network it will perform
worse. But in fact, it's the opposite: by giving a network many related tasks
like we did above, will help the network to produce better predictions.</p>
</div>

			</div>
		</article>
	</body>
	</html>
	