
	<!DOCTYPE html>
	<html>
	<head>
		<meta charset="UTF-8">

		<!--
		<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/meyer-reset/2.0/reset.min.css" />
		<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/normalize/7.0.0/normalize.min.css" />
		-->

		<link rel="stylesheet" href="/node_sources/part-x/index-v1.css">
		<link rel="stylesheet" href="/node_modules/katex/dist/katex.min.css">
		<link rel="stylesheet" href="/node_modules/highlight.js/styles/default.css">

		<link rel="stylesheet" href="/node_modules/slick-carousel/slick/slick.css">
		<link rel="stylesheet" href="/node_modules/slick-carousel/slick/slick-theme.css">

		<script src="/node_modules/lodash/lodash.min.js"></script>
		<script src="/node_modules/highcharts/highcharts.js"></script>
		<script src="/node_modules/jquery/dist/jquery.min.js"></script>
		<script src="/node_modules/slick-carousel/slick/slick.min.js"></script>

		<link rel="stylesheet" href="//code.jquery.com/ui/1.12.1/themes/base/jquery-ui.css">
		<script src="https://code.jquery.com/ui/1.12.1/jquery-ui.js"></script>
	</head>
	<body>
		<div id="content">
			<script src="/node_sources/part-1/images-html/train/invsign.js"></script>
<h1>Gradient Descent: the magic</h1>
<p>Gradient descent is an algorithm used to train networks. It's a key component
of machine learning and we are about to learn how it works !</p>
<p>In order to keep the math simple, we will use a new problem: given a number,
output that same number with its sign reversed. We will also use a small dataset
comprised of only two values:</p>
<ul>
<li><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>x</mi><mo>=</mo><mo>+</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">x = +1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.64444em;"></span><span class="strut bottom" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="base textstyle uncramped"><span class="mord mathit">x</span><span class="mrel">=</span><span class="mord">+</span><span class="mord mathrm">1</span></span></span></span>, <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>y</mi><mo>=</mo><mo>−</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">y = -1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.64444em;"></span><span class="strut bottom" style="height:0.8388800000000001em;vertical-align:-0.19444em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.03588em;">y</span><span class="mrel">=</span><span class="mord">−</span><span class="mord mathrm">1</span></span></span></span></li>
<li><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>x</mi><mo>=</mo><mo>−</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">x = -1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.64444em;"></span><span class="strut bottom" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="base textstyle uncramped"><span class="mord mathit">x</span><span class="mrel">=</span><span class="mord">−</span><span class="mord mathrm">1</span></span></span></span>, <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>y</mi><mo>=</mo><mo>+</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">y = +1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.64444em;"></span><span class="strut bottom" style="height:0.8388800000000001em;vertical-align:-0.19444em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.03588em;">y</span><span class="mrel">=</span><span class="mord">+</span><span class="mord mathrm">1</span></span></span></span></li>
</ul>
<div class="note">
<p>This is not a classification problem but a regression problem.</p>
</div>
<p>For now, let's forget about activations and biases: a single neuron is enought to
solve this problem.</p>
<p><img src="/node_outputs/part-1/images-js/nnet-sign.png" alt=""></p>
<p>And here is what we expect:</p>
<p><img src="/node_outputs/part-1/images-js/nnet-sign-dataset.png" alt=""></p>
<p>As said before, training a network is just finding finding the good weights. So
let's do that !</p>
<p>First we need to initialize our network: we need to give the weight an initial
value. There is many different methods to do so, but we have no interest in them
in this post. So let's use an arbitrary value:</p>
<p><img src="/node_outputs/part-1/images-js/nnet-sign-init.png" alt=""></p>
<p>Once initialized, we can use our network to make some predictions:</p>
<p><img src="/node_outputs/part-1/images-js/nnet-sign-init-dataset.png" alt=""></p>
<p>And as expected, this is not what we wanted. (todo: center, labels, title &amp;
tooltips...)</p>
<script>renderInvsign(+2, undefined, false)</script>
<p>So, how bad are our predictions ? We need a way to know how wrong we are. One
way to do that is to compute the distances between our predictions and the
expected outputs: subtract the labels to our predictions.
(rephrase...)</p>
<p><img src="/node_outputs/part-1/images-js/nnet-sign-init-error-sub.png" alt=""></p>
<p>The distances we compute correspond to the red lines on the figure below:</p>
<script>renderInvsign(+2)</script>
<p>We can see that the distances can be either positives or negatives. But we don't
care about the sign: we are only interested by how much. So we square the result
to get positive distances only.</p>
<p><img src="/node_outputs/part-1/images-js/nnet-sign-init-error.png" alt=""></p>
<p>Finally, let's sum all the errors so that we can have a single value
representing the total error of the network: <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>9</mn><mo>+</mo><mn>9</mn><mo>=</mo><mn>1</mn><mn>8</mn></mrow><annotation encoding="application/x-tex">9 + 9 = 18</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.64444em;"></span><span class="strut bottom" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="base textstyle uncramped"><span class="mord mathrm">9</span><span class="mbin">+</span><span class="mord mathrm">9</span><span class="mrel">=</span><span class="mord mathrm">1</span><span class="mord mathrm">8</span></span></span></span>.</p>
<p>Let's plot it (add distance with weight &amp; color current weight)</p>
<script>renderInvsignModel(+2)</script>
<p>Ok, now we know how to compute the error of our network, what are we going to
use it for ?</p>
<p>Let's take some time to experiment: what would happen if we used a bigger weight
?</p>
<p><img src="/node_outputs/part-1/images-js/nnet-sign-init-error-inc.png" alt=""></p>
<p>The error would go up: <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>1</mn><mn>6</mn><mo>+</mo><mn>1</mn><mn>6</mn><mo>=</mo><mn>3</mn><mn>2</mn></mrow><annotation encoding="application/x-tex">16 + 16 = 32</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.64444em;"></span><span class="strut bottom" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="base textstyle uncramped"><span class="mord mathrm">1</span><span class="mord mathrm">6</span><span class="mbin">+</span><span class="mord mathrm">1</span><span class="mord mathrm">6</span><span class="mrel">=</span><span class="mord mathrm">3</span><span class="mord mathrm">2</span></span></span></span>. We can also see that the distance between
the predictions and the labels increased: our network makes worse predictions.</p>
<script>renderInvsignModel(+3)</script>
<p>What if we used a smaller weight ?</p>
<p><img src="/node_outputs/part-1/images-js/nnet-sign-init-error-dec.png" alt=""></p>
<p>The error would go down: <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>4</mn><mo>+</mo><mn>4</mn><mo>=</mo><mn>8</mn></mrow><annotation encoding="application/x-tex">4 + 4 = 8</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.64444em;"></span><span class="strut bottom" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="base textstyle uncramped"><span class="mord mathrm">4</span><span class="mbin">+</span><span class="mord mathrm">4</span><span class="mrel">=</span><span class="mord mathrm">8</span></span></span></span>. The generated function pass closer to our
dataset points: our network makes better predictions.</p>
<script>renderInvsignModel(+1)</script>
<p>Reducing the weight improved our predictions, let's do that again !</p>
<script>renderInvsignModel(+0)</script>
<p>And once again our error decreased ! Let's not stop here !</p>
<script>renderInvsignModel(-1)</script>
<p>Our error is now 0: we just found the perfect weight for this problem: the
function fit perfectly our dataset.</p>
<p>What happens if we keep decreasing the weight ? Well, from this point, the error
will get bigger and bigger.</p>
<script src="/node_sources/part-1/images-html/train/invsign-error-animated.js"></script>
<div class="important">
<p>the lower the error, the better our network fit our dataset.</p>
</div>
<p>Now that we understand how to compute errors and what they represent, let's draw
all of them:</p>
<script src="/node_sources/part-1/images-html/train/invsign-error.js"></script>
<p>We can see that the curve has a bowl like shape. Let's imagine that this curve
represent a hill. You're gone for a hike and you're lost: you are currently at a
random position on the hill (the initial weight value).</p>
<script src="/node_sources/part-1/images-html/train/invsign-descent.js"></script>
<script>renderInvsignDescent(+2.0)</script>
<p>You want to go back to your home (the perfect weight) but unfortunately there is
fog: you can't see your house nor know where you are on the hill. Fortunately,
you know your house is at the bottom of the hill and you can see enought to know
the direction of the slope. So you look around and take step towards the bottom
(you improve your weight). (TODO: hide part of the curve and explain lr ?)</p>
<script>renderInvsignDescent(+1.5)</script>
<p>And you repeat this, again and again, until you reach your house (the perfect
weight value) (todo: slope in red)</p>
<script src="/node_sources/part-1/images-html/train/invsign-descent-animated.js"></script>
<p>No matter where you start, by following the direction of the slope, you will
eventually reach the bottom.</p>
<p>And that's exactly how our algorithm works ! This is why it's called gradient
descent (gradient is another word for slope).</p>
<p>Now we got the big picture, let's implement it !</p>
<h2>Math</h2>
<p>The algorithm in itself is just a big loop. The tricky part is getting the
direction of the slope at our current position.</p>
<p>Fortunately for us there is a mathematical tool that just do that: the
derivatives ! Let's take some time to understand how they work.</p>
<div class="note">
<p>Computing the exact values of the derivatives would just make things more
complicated. Instead we will use an approximation that's easier to understand
and gives us almost the same results.</p>
</div>
<p>As said before a derivative compute a slope. So let's review how we compute a
slope between two points <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>a</mi></mrow><annotation encoding="application/x-tex">a</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">a</span></span></span></span> and <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>b</mi></mrow><annotation encoding="application/x-tex">b</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.69444em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">b</span></span></span></span>. As a convention <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>b</mi></mrow><annotation encoding="application/x-tex">b</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.69444em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">b</span></span></span></span> is on the right of <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>a</mi></mrow><annotation encoding="application/x-tex">a</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">a</span></span></span></span>.</p>
<ul>
<li>measure the altitude between both points: <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>y</mi><mi>b</mi><mo>−</mo><mi>y</mi><mi>a</mi></mrow><annotation encoding="application/x-tex">yb - ya</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.03588em;">y</span><span class="mord mathit">b</span><span class="mbin">−</span><span class="mord mathit" style="margin-right:0.03588em;">y</span><span class="mord mathit">a</span></span></span></span></li>
<li>then divide by the distance between the two points: <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>x</mi><mi>b</mi><mo>−</mo><mi>x</mi><mi>a</mi></mrow><annotation encoding="application/x-tex">xb - xa</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.77777em;vertical-align:-0.08333em;"></span><span class="base textstyle uncramped"><span class="mord mathit">x</span><span class="mord mathit">b</span><span class="mbin">−</span><span class="mord mathit">x</span><span class="mord mathit">a</span></span></span></span></li>
</ul>
<p>(add lines...)</p>
<p><img src="/node_outputs/part-1/images-js/gd-graph-slope-neg.png" alt=""></p>
<p>If the result is negative, it means that if you go from <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>a</mi></mrow><annotation encoding="application/x-tex">a</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">a</span></span></span></span> to <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>b</mi></mrow><annotation encoding="application/x-tex">b</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.69444em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">b</span></span></span></span>, you go down.</p>
<p><img src="/node_outputs/part-1/images-js/gd-graph-slope-pos.png" alt=""></p>
<p>If the result is positive, it means that if you go from <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>a</mi></mrow><annotation encoding="application/x-tex">a</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">a</span></span></span></span> to <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>b</mi></mrow><annotation encoding="application/x-tex">b</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.69444em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">b</span></span></span></span>, you go up.</p>
<p><img src="/node_outputs/part-1/images-js/gd-graph-slope-pos-steep.png" alt=""></p>
<p>The bigger the value, the steeper the slope</p>
<p>Now there is a little problem, our derivatives compute the slope at a given
point: there is no second point ! In fact a derivative compute the slope between
the given point and a second very very close point. So given a point <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">x</span></span></span></span>, we can
approximate the derivative as:</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>d</mi><mi>e</mi><mi>r</mi><mi>i</mi><mi>v</mi><mi>a</mi><mi>t</mi><mi>i</mi><mi>v</mi><mi>e</mi><mo>(</mo><mi>x</mi><mo>)</mo><mo>=</mo><mi>s</mi><mi>l</mi><mi>o</mi><mi>p</mi><mi>e</mi><mo>(</mo><mi>x</mi><mo separator="true">,</mo><mi>x</mi><mo>+</mo><mn>0</mn><mi mathvariant="normal">.</mi><mn>0</mn><mn>0</mn><mn>0</mn><mn>1</mn><mo>)</mo></mrow><annotation encoding="application/x-tex">derivative(x) = slope(x, x + 0.0001)
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base displaystyle textstyle uncramped"><span class="mord mathit">d</span><span class="mord mathit">e</span><span class="mord mathit" style="margin-right:0.02778em;">r</span><span class="mord mathit">i</span><span class="mord mathit" style="margin-right:0.03588em;">v</span><span class="mord mathit">a</span><span class="mord mathit">t</span><span class="mord mathit">i</span><span class="mord mathit" style="margin-right:0.03588em;">v</span><span class="mord mathit">e</span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mclose">)</span><span class="mrel">=</span><span class="mord mathit">s</span><span class="mord mathit" style="margin-right:0.01968em;">l</span><span class="mord mathit">o</span><span class="mord mathit">p</span><span class="mord mathit">e</span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mpunct">,</span><span class="mord mathit">x</span><span class="mbin">+</span><span class="mord mathrm">0</span><span class="mord mathrm">.</span><span class="mord mathrm">0</span><span class="mord mathrm">0</span><span class="mord mathrm">0</span><span class="mord mathrm">1</span><span class="mclose">)</span></span></span></span></span></p>
<p>Another way to think about derivatives is: given a point, how will the output
change if i move a tiny bit to the right ?</p>
<h1>code</h1>
<p>It's time to implement what we've seen ! Let's begin with our usual neuron
function.</p>
<pre><code class="language-python"><span class="hljs-comment"># initial weight</span>
weight = <span class="hljs-number">2</span>

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">neuron</span><span class="hljs-params">(x, w)</span>:</span>
    <span class="hljs-keyword">return</span> x * w

neuron(<span class="hljs-number">-1</span>, weight)

<span class="hljs-comment">#&gt; -2</span>

neuron(+<span class="hljs-number">1</span>, weight)

<span class="hljs-comment">#&gt; +2</span>
</code></pre>
<p>Next we compute the error for a single prediction.</p>
<pre><code class="language-python"><span class="hljs-comment"># our dataset</span>
inputs = [<span class="hljs-number">-1</span>, +<span class="hljs-number">1</span>]
labels = [+<span class="hljs-number">1</span>, <span class="hljs-number">-1</span>]

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">neuron_error</span><span class="hljs-params">(prediction, label)</span>:</span>
    <span class="hljs-string">"""
    distance between our prediction and the label
    """</span>
    <span class="hljs-keyword">return</span> (prediction - label) ** <span class="hljs-number">2</span>

p = neuron(inputs[<span class="hljs-number">0</span>], weight)
neuron_error(p, labels[<span class="hljs-number">0</span>])

<span class="hljs-comment">#&gt; 9</span>

p = neuron(inputs[<span class="hljs-number">1</span>], weight)
neuron_error(p, labels[<span class="hljs-number">1</span>])

<span class="hljs-comment">#&gt; 9</span>
</code></pre>
<p>Then we compute the total error of a network given some weights.</p>
<pre><code class="language-python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">network_error</span><span class="hljs-params">(weight)</span>:</span>
    <span class="hljs-string">"""
    for each input in our dataset
        predict and compute the error
    return the sum of the errors
    """</span>
    error = <span class="hljs-number">0</span>
    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(len(inputs)):
        p      = neuron(inputs[i], weight)
        error += neuron_error(p, labels[i])
    <span class="hljs-keyword">return</span> error

network_error(weight)

<span class="hljs-comment">#&gt; 18</span>
</code></pre>
<p>The next function compute the slope between two network errors. The two points
<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">x</span></span></span></span> are two weights and their value <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.03588em;">y</span></span></span></span> is their corresponding network error.</p>
<pre><code class="language-python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">slope</span><span class="hljs-params">(wa, wb)</span>:</span>
    xa = wa
    xb = wb
    ya = network_error(xa)
    yb = network_error(xb)
    <span class="hljs-keyword">return</span> (yb - ya) / (xb - xa)
</code></pre>
<p>And now we implement the derivative that give us the slope for a given weight
value. Remember that a derivative is almost equivalent to computing the slope
between the given point and a very close point.</p>
<pre><code class="language-python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">derivative</span><span class="hljs-params">(w)</span>:</span>
    <span class="hljs-keyword">return</span> slope(w, w + <span class="hljs-number">0.0001</span>)
</code></pre>
<p>Let's check that our function works by getting the slope at our initial weight
value:</p>
<pre><code class="language-python">derivative(weight)

<span class="hljs-comment">#&gt; 12.000199999998785</span>
</code></pre>
<p>We can see it's positive, i.e that if we increase the weight, the error increase
too. Exactly what we could observe with the error curve.</p>
<p>Now, let's fit all the pieces together ! If the derivative is positive, that
means if we go right (increase the weight) the error increase. In this case we
would like to decrease the weight to reduce the error:</p>
<p><img src="/node_outputs/part-1/images-js/gd-graph-slope-pos.png" alt=""></p>
<p>If the derivative is negative, that means if we go right (increase the weight)
the error decrease.</p>
<p><img src="/node_outputs/part-1/images-js/gd-graph-slope-neg.png" alt=""></p>
<p>Let's summarize:</p>
<ul>
<li>derivative is positive -&gt; decrease the weight</li>
<li>derivative is negative -&gt; increase the weight</li>
</ul>
<p>One simple way to do that is to subtract the value of the derivative from the
weight.</p>
<ul>
<li>derivative = +1 -&gt; w - (+1) = w - 1 -&gt; decrease the weight</li>
<li>derivative = -1 -&gt; w - (-1) = w + 1 -&gt; increase the weight</li>
</ul>
<p>Implementing the step function is pretty straightforward:</p>
<pre><code class="language-python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">step</span><span class="hljs-params">(w)</span>:</span>
    s  = derivative(w)
    w -= s
    <span class="hljs-keyword">return</span> w
</code></pre>
<p>Now let's take a step, we have currently <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>w</mi><mo>=</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">w = 2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.64444em;"></span><span class="strut bottom" style="height:0.64444em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.02691em;">w</span><span class="mrel">=</span><span class="mord mathrm">2</span></span></span></span> and we know that the optimal
weight is <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>w</mi><mo>=</mo><mo>−</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">w = -1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.64444em;"></span><span class="strut bottom" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.02691em;">w</span><span class="mrel">=</span><span class="mord">−</span><span class="mord mathrm">1</span></span></span></span>. So a step should decrease the weight to make it closer to
-1.</p>
<pre><code class="language-python">weight

<span class="hljs-comment">#&gt; 2</span>

weight = step(weight)
weight

<span class="hljs-comment">#&gt; -10</span>
</code></pre>
<p>Wow, what happened ? We wanted to take small steps until we reached -1, the
perfect weight, but instead we took a giant step, and we overshoot the bottom.</p>
<script src="/node_sources/part-1/images-html/train/invsign-overshoot.js"></script>
<p>TODO: explain why ? big/small steps and why steps</p>
<p>To prevent that we use what's called a learning rate. It's a number we multiply
with the derivative value that allows us to control the step size. In this case
we would like to take smaller steps, so we use a small number:</p>
<pre><code class="language-python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">step</span><span class="hljs-params">(w, learning_rate = <span class="hljs-number">0.01</span>)</span>:</span>
    s  = derivative(w)
    w -= s * learning_rate
    <span class="hljs-keyword">return</span> w
</code></pre>
<p>Now let's start again</p>
<pre><code class="language-python">weight = <span class="hljs-number">2</span>
weight = step(weight)
weight

<span class="hljs-comment">#&gt; 1.8</span>
</code></pre>
<p>That's better, so let's take 100 more steps:</p>
<pre><code class="language-python"><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">100</span>):
    weight = step(weight)

weight

<span class="hljs-comment">#&gt; -0.95</span>
</code></pre>
<p>Nice almost the good weight -1. Now you understand why our network never find
the exact perfect weight. We have to take small steps, and we can only become
closer and closer to it without ever reaching it.</p>
<p>Now let's see our network prediction now:</p>
<pre><code class="language-python">neuron(<span class="hljs-number">-1</span>, weight)

<span class="hljs-comment">#&gt; 0.95</span>

neuron(+<span class="hljs-number">1</span>, weight)

<span class="hljs-comment">#&gt; -0.95</span>
</code></pre>
<p>And this is it ! We could run a few more steps to have a better accuracy, but
its not very interesting. (TODO: stop when you have satisfying results)</p>
<p>You will often hear that neural networks are functions approximator. It just
refer to what we have seen: neural networks represent a function, and each time
we improve the weights the function fit better the dataset but never exactly. So
they approximate the desired function.</p>
<h2>Gradient descent: more</h2>
<p>Ok, now we know of how to train a small network with only one weight, but how
does this work with more complex networks that have multiple inputs, neurons,
biases and layers ? The only thing that changes, is that you now have more
weights. For each one, you repeat the same procedure we did: how does the error
change if the weight is increased a bit ? Then update the weight to decrease the
error.</p>
<p>To compute the slope for a specific weight, freeze all other weights. Here is an
example for computing the derivative of the second weight of a network with 4
weights:</p>
<pre><code class="language-python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">derivative_w2</span><span class="hljs-params">(w1, w2, w3, w4)</span>:</span>
    xa = w2
    xb = w2 + <span class="hljs-number">0.0001</span>

    yb = network_error(w1, xb, w3, w4)
    ya = network_error(w1, xa, w3, w4)

    <span class="hljs-keyword">return</span> (yb - ya) / (xb - xa)
</code></pre>
<p>Compute the update for all the weights then update them all at once.</p>
<p>talk about 2d/3d</p>
<p>Computing the derivative for each weights is heavy. Instead we use a method
called backpropagation, that reuse previous derivative to speed things up. It
begins by computing the derivatives of the last layer and then use those to
compute the derivatives of the previous layer etc.... This is called the
backward pass because it goes from right to left in contrast with the forward
pass. (forward = prediction, backward = training)</p>
<p><img src="/node_outputs/part-1/images-js/nnet-2332-backward.png" alt=""></p>
<p>Now what if we have activations ? well, activations have no weights, so there is
nothing to compute. But remember when we said activations had some constraints ?
Well one of them is that the 'hill' shape must be preserved. A bad activation
will output a curve that won't work with our algorithm. Here is such a case:</p>
<script src="/node_sources/part-1/images-html/train/step.js"></script>
<p>The slope is 0 almost everywhere: you will never know in which direction you
must update your weight.</p>
<p>But as you'll be using well known activations, you will never have this problem.</p>
<p>You just got an intuition behind one of the most important algorithm in machine
learning ! It's so important that I'll cover it in more details in a dedicated
post later in this series.</p>
<p>Now, let's take a last detour to wrap things up before doing the final project.
The hardest part is behind you, almost there =)</p>
<p>TODO: little summary</p>

		</div>
	</body>
	</html>
	