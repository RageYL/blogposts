
	<!DOCTYPE html>
	<html>
	<head>
		<meta charset="UTF-8">

		<!--
		<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/meyer-reset/2.0/reset.min.css" />
		<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/normalize/7.0.0/normalize.min.css" />
		-->

		<link rel="stylesheet" href="/node_sources/part-x/index-v1.css">
		<link rel="stylesheet" href="/node_modules/katex/dist/katex.min.css">
		<link rel="stylesheet" href="/node_modules/highlight.js/styles/default.css">

		<link rel="stylesheet" href="/node_modules/slick-carousel/slick/slick.css">
		<link rel="stylesheet" href="/node_modules/slick-carousel/slick/slick-theme.css">

		<script src="/node_modules/lodash/lodash.min.js"></script>
		<script src="/node_modules/highcharts/highcharts.js"></script>
		<script src="/node_modules/jquery/dist/jquery.min.js"></script>
		<script src="/node_modules/slick-carousel/slick/slick.min.js"></script>

		<link rel="stylesheet" href="//code.jquery.com/ui/1.12.1/themes/base/jquery-ui.css">
		<script src="https://code.jquery.com/ui/1.12.1/jquery-ui.js"></script>
	</head>
	<body>
		<div id="content">
			<script src="/node_sources/part-1/images-html/activ/activ.js"></script>
<h1>Activation functions</h1>
<p>Activation functions is a name given to functions applied to the output of
neurons.</p>
<p>Without activation, we sum the products of each input with its corresponding
weight.</p>
<p><img src="/node_outputs/part-1/images-js/nnet-21-equation.png" alt=""></p>
<p>Here is the corresponding computation graph:</p>
<p><img src="/node_outputs/part-1/images-js/nnet-21-computation.png" alt=""></p>
<p>With an activation, we also apply a function on the result:</p>
<p><img src="/node_outputs/part-1/images-js/nnet-21-equation-activ.png" alt=""></p>
<p><img src="/node_outputs/part-1/images-js/nnet-21-computation-activ.png" alt=""></p>
<h2>Code</h2>
<p>Enought words, let's code. In the first example we will create an activation
that do nothing, the result will be the same as without an activation.</p>
<pre><code class="language-python">w1 = <span class="hljs-number">0.4</span>
w2 = <span class="hljs-number">0.4</span>

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">neuron</span><span class="hljs-params">(x1, x2)</span>:</span>
    z  = x1 * w1
    z += x2 * w2
    <span class="hljs-keyword">return</span> z

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">do_nothing</span><span class="hljs-params">(z)</span>:</span>
    <span class="hljs-keyword">return</span> z

do_nothing(neuron(BLACK, WHITE))

<span class="hljs-comment">#&gt; 0.4</span>

do_nothing(neuron(WHITE, WHITE))

<span class="hljs-comment">#&gt; 0.8</span>
</code></pre>
<p>In the second example we will use an activation that square the output.</p>
<pre><code class="language-python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">square</span><span class="hljs-params">(z)</span>:</span>
    <span class="hljs-keyword">return</span> z * z

square(neuron(BLACK, WHITE))

<span class="hljs-comment">#&gt; 0.16</span>

square(neuron(WHITE, WHITE))

<span class="hljs-comment">#&gt; 0.64</span>
</code></pre>
<p>With keras there is two way to use activations. The first one is by using an
activation layer:</p>
<pre><code class="language-python"><span class="hljs-keyword">from</span> keras.layers <span class="hljs-keyword">import</span> Activation
</code></pre>
<p>It takes a single argument: the function to apply.</p>
<pre><code class="language-python">model = Sequential()
model.add(Dense(<span class="hljs-number">1</span>, use_bias = <span class="hljs-keyword">False</span>, input_shape = (<span class="hljs-number">2</span>,))
model.add(Activation(square))
</code></pre>
<p>The second, shorter version, is to use the <code>activation</code> parameter of a layer.</p>
<pre><code class="language-python">model = Sequential()
model.add(Dense(<span class="hljs-number">1</span>, use_bias = <span class="hljs-keyword">False</span>, input_shape = (<span class="hljs-number">2</span>,), activation = square))
</code></pre>
<p>If we reuse the weights of our previous example, we get the same predictions</p>
<pre><code class="language-python">model.set_weights([[[<span class="hljs-number">0.4</span>], [<span class="hljs-number">0.4</span>]]])

model.predict(np.array([[BLACK, WHITE]]))

<span class="hljs-comment">#&gt; array([[ 0.16 ]], dtype=float32)</span>

model.predict(np.array([[WHITE, WHITE]]))

<span class="hljs-comment">#&gt; array([[ 0.64 ]], dtype=float32)</span>
</code></pre>
<p>In both case the activation function is applied on all the neurons of the
previous layer.</p>
<pre><code class="language-python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">ActivationLayer</span><span class="hljs-params">(object)</span>:</span>

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, activation)</span>:</span>
        self.activation = activation

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">predict</span><span class="hljs-params">(self, inputs)</span>:</span>
        <span class="hljs-string">"""
        For each inputs
            apply the activation
        return all outputs
        """</span>
        outputs = []
        <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> inputs:
            outputs.append(self.activation(x))
        <span class="hljs-keyword">return</span> outputs
</code></pre>
<p>We now understand what are activation functions, but how do they help us ?</p>
<h1>Activation with hidden layers</h1>
<p>Remember our problem ? Due to our neurons producing linear outputs, adding more
neurons had no effect. Let's look at the output of a single neuron with a single
input.</p>
<p><img src="/node_outputs/part-1/images-js/nnet-11-linear.png" alt=""></p>
<script src="/node_sources/part-1/images-html/activ/nnet-11-linear.js"></script>
<p>If we use a non-linear activation, the output is no longer linear:</p>
<p><img src="/node_outputs/part-1/images-js/nnet-11-hidden-activ.png" alt=""></p>
<script src="/node_sources/part-1/images-html/activ/nnet-11.js"></script>
<p>Now if we stack one more neuron, we can learn a more complex, non linear function:</p>
<p><img src="/node_outputs/part-1/images-js/nnet-111-hidden-activ.png" alt=""></p>
<script src="/node_sources/part-1/images-html/activ/nnet-111.js"></script>
<p>And like that we can build big networks. This may seems like a small change but
this trick was the missing piece to learn arbitrary complex functions. Given
enought neurons, it has been proven that a network with a single hidden layer
and non-linear activation can represent any function. (search 'universal
approximation theorem' for more information)</p>
<p>But not all functions can be used as activation, there is some restrictions.
Fortunately we don't need to know them: In machine learning we juggle with a few
well studied activations, we just need to learn when to use them.</p>
<p>TODO: can we still learn linear function ?</p>
<h1>ReLu</h1>
<p>Nowadays, for hidden layers, a strong default is the Rectified Linear Unit or
ReLu for short.</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>m</mi><mi>i</mi><mi>n</mi><mo>(</mo><mn>0</mn><mo separator="true">,</mo><mi>x</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">min(0, x)
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base displaystyle textstyle uncramped"><span class="mord mathit">m</span><span class="mord mathit">i</span><span class="mord mathit">n</span><span class="mopen">(</span><span class="mord mathrm">0</span><span class="mpunct">,</span><span class="mord mathit">x</span><span class="mclose">)</span></span></span></span></span></p>
<script src="/node_sources/part-1/images-html/activ/relu.js"></script>
<p>With keras, we can also pass a string instead of a function for preregistered
activation.</p>
<pre><code class="language-python">model.add(Activation(<span class="hljs-string">"relu"</span>))
<span class="hljs-comment"># or</span>
model.add(Dense(..., activation = <span class="hljs-string">"relu"</span>))
</code></pre>
<h1>Activation with output layers</h1>
<p>Apart from being used on hidden layers to be able to build complex networks, activations
can also be used on output layers to format output values.</p>
<p>For example, for classification tasks we almost always want the output to be between 0
and 1: we don't care about other values. But currently, without activation, a
neuron can return any value.</p>
<p>There is an activation called sigmoid, that can help us solve this problem, so
let's take a look at it.</p>
<h1>Sigmoid</h1>
<p>(margin &amp; issue with exponent)</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mfrac><mrow><mn>1</mn></mrow><mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi><mrow><mo>âˆ’</mo><mi>x</mi></mrow></msup></mrow></mfrac></mrow><annotation encoding="application/x-tex">\dfrac{1}{1 + e^{-x}}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:1.32144em;"></span><span class="strut bottom" style="height:2.09077em;vertical-align:-0.7693300000000001em;"></span><span class="base displaystyle textstyle uncramped"><span class="mord reset-textstyle displaystyle textstyle uncramped"><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span><span class="mfrac"><span class="vlist"><span style="top:0.686em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">â€‹</span></span><span class="reset-textstyle textstyle cramped"><span class="mord textstyle cramped"><span class="mord mathrm">1</span><span class="mbin">+</span><span class="mord"><span class="mord mathit">e</span><span class="vlist"><span style="top:-0.289em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">â€‹</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord">âˆ’</span><span class="mord mathit">x</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">â€‹</span></span>â€‹</span></span></span></span></span></span><span style="top:-0.22999999999999998em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">â€‹</span></span><span class="reset-textstyle textstyle uncramped frac-line"></span></span><span style="top:-0.677em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">â€‹</span></span><span class="reset-textstyle textstyle uncramped"><span class="mord textstyle uncramped"><span class="mord mathrm">1</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">â€‹</span></span>â€‹</span></span></span><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span></span></span></span></span></span></p>
<script src="/node_sources/part-1/images-html/activ/sigmoid.js"></script>
<p>As we can see, its output is constrained between 0 and 1, so we will never get
undesirable values.</p>
<p>The sigmoid function was the de-facto activation used everywhere a few years
ago. Nowadays we mostly use it when we need an output between 0 and 1. (~~~)</p>
<p>For now, this activation will be enought, so let's move on.</p>
<h1>Code</h1>
<p>We now have learned enought to be able to build powerful networks. So let's put
this knowledge into practice by building a useful model that detect edges: given
two pixels:</p>
<ul>
<li>return 1 if both are of different colors</li>
<li>return 0 otherwise</li>
</ul>
<p>First let's confirm we couldn't solve this problem without activations.</p>
<p>TODO: visualize function as non linear 1d concat or 3d</p>
<p>Now, let's think about which activations we want use. As said before, for hidden
layers, ReLu is currently the preferred choice. As for the output we want to
return 1 if there is a border and 0 otherwise. So we can use a sigmoid
activation to avoid output values like 5 or -2.</p>
<p>There is no easy way to choose the number of neurons and layers, we have to try
some configurations. Here this is a simple problem, so we don't need a big
network. I tried 1 layer with 8 neurons and it worked, so let's use that.</p>
<p>For the output, we only have one category (border or not), so one output neuron
is enought.</p>
<p>TODO: picture ?</p>
<p>Here is the code, you should be familiar with:</p>
<pre><code class="language-python">border_m = Sequential()
border_m.add(Dense(<span class="hljs-number">8</span>, activation = <span class="hljs-string">"relu"</span>, input_shape = (<span class="hljs-number">2</span>,)))
border_m.add(Dense(<span class="hljs-number">1</span>, activation = <span class="hljs-string">"sigmoid"</span>))

border_x = [[BLACK, BLACK], [WHITE, BLACK], [WHITE, WHITE], [BLACK, WHITE]]
border_y = [<span class="hljs-number">0</span>,              <span class="hljs-number">1</span>,              <span class="hljs-number">0</span>,              <span class="hljs-number">1</span>]

train(
    border_m,
    border_x,
    border_y
)

<span class="hljs-comment">#&gt; ...</span>
<span class="hljs-comment">#&gt; Epoch 1999/2000</span>
<span class="hljs-comment">#&gt; 4/4 [==============================] - 0s - loss: 0.1216 - acc: 1.0000</span>
<span class="hljs-comment">#&gt; Epoch 2000/2000</span>
<span class="hljs-comment">#&gt; 4/4 [==============================] - 0s - loss: 0.1215 - acc: 1.0000</span>

border_m.predict_classes(border_x)

<span class="hljs-comment">#&gt; array([[0],</span>
<span class="hljs-comment">#&gt;        [1],</span>
<span class="hljs-comment">#&gt;        [0],</span>
<span class="hljs-comment">#&gt;        [1]], dtype=int32)</span>

border_m.predict(border_x)

<span class="hljs-comment">#&gt; array([[ 0.42065826],</span>
<span class="hljs-comment">#&gt;        [ 0.63238412],</span>
<span class="hljs-comment">#&gt;        [ 0.33127135],</span>
<span class="hljs-comment">#&gt;        [ 0.74696058]], dtype=float32)</span>
</code></pre>
<p>And it works ! As usual, let's code what happened, we will see that there is no
black magic involved.</p>
<p>TODO:</p>
<pre><code class="language-python">w1 = border_m.layers[<span class="hljs-number">0</span>].get_weights()[<span class="hljs-number">0</span>]
b1 = border_m.layers[<span class="hljs-number">0</span>].get_weights()[<span class="hljs-number">1</span>]
w2 = border_m.layers[<span class="hljs-number">1</span>].get_weights()[<span class="hljs-number">0</span>]
b2 = border_m.layers[<span class="hljs-number">1</span>].get_weights()[<span class="hljs-number">1</span>]

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">dense</span><span class="hljs-params">(n, x, w, b, activation)</span>:</span>
    z = np.dot(x, w) + b
    a = activation(z)
    <span class="hljs-keyword">return</span> z

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">relu</span><span class="hljs-params">(x)</span>:</span>
    <span class="hljs-keyword">return</span> np.maximum(x, <span class="hljs-number">0</span>)

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">sigmoid</span><span class="hljs-params">(x)</span>:</span>
    <span class="hljs-keyword">return</span> <span class="hljs-number">1</span> / (<span class="hljs-number">1</span> + np.exp(-x))

x = [BLACK, WHITE]
x = dense(<span class="hljs-number">8</span>, x, w1, b1, relu)
y = dense(<span class="hljs-number">1</span>, x, w2, b2, sigmoid)
y

<span class="hljs-comment">#&gt; array([ 0.74696058])</span>
</code></pre>
<h1>Designing bigger networks</h1>
<p>Let's end this section with some words about designing neural nets.</p>
<p>The first thing to know is that nobody knows the best number of neurons and
layers for your network. You always have to try different architectures and
keep the one that work best for your problem.</p>
<p>Now, why use layers at all ? As said before, it has been proven that a network
with a single hidden layer can approximate any function, so why not just add
more neurons and forget about layers ? Well, while it can learn anything, it
doesn't mean it's efficient.  We've seen that neurons can compute simple
functions, so to perform a complex tasks like recognizing an animal in a
picture, you need lots and lots of neurons.
But if instead you use multiple layers, you can make the problem much more
easier:</p>
<ul>
<li>the 1st layer detect the edges and the colors</li>
<li>the 2nd layer use the edges to detect more complex shapes like lines,
squares, circles...</li>
<li>the 3rd layer can now learn to recognize eyes, legs, arms...</li>
<li>etc...</li>
</ul>
<p>Using more layers will require far less neurons and learn more quickly and
accurately. Note that you still need to have enought neurons on a layer so that
subsequent layers have enought data to work on.</p>
<p>So now, the question you may ask is why don't we go deeper and deeper ? Well
training deep networks (deep learning) is not easy. Until a few years ago it was
very hard to train networks with more than a few layers and we didn't have the
computation power to do it. But thanks to the rise in computing capabilities and
research in machine learning, we can now train bigger networks that can have
hundreds of layers. As we will see deep learning, is basically just tricks to
make neural networks work better and faster.</p>

		</div>
	</body>
	</html>
	