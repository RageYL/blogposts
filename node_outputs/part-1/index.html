
	<!DOCTYPE html>
	<html>
	<head>
		<meta charset="UTF-8">

		<!--
		<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/meyer-reset/2.0/reset.min.css" />
		<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/normalize/7.0.0/normalize.min.css" />
		-->

		<link rel="stylesheet" href="/node_sources/part-x/index-v1.css">
		<link rel="stylesheet" href="/node_modules/katex/dist/katex.min.css">
		<link rel="stylesheet" href="/node_modules/highlight.js/styles/default.css">

		<link rel="stylesheet" href="/node_modules/slick-carousel/slick/slick.css">
		<link rel="stylesheet" href="/node_modules/slick-carousel/slick/slick-theme.css">

		<script src="/node_modules/lodash/lodash.min.js"></script>
		<script src="/node_modules/highcharts/highcharts.js"></script>
		<script src="/node_modules/jquery/dist/jquery.min.js"></script>
		<script src="/node_modules/slick-carousel/slick/slick.min.js"></script>

		<link rel="stylesheet" href="//code.jquery.com/ui/1.12.1/themes/base/jquery-ui.css">
		<script src="https://code.jquery.com/ui/1.12.1/jquery-ui.js"></script>
	</head>
	<body>
		<div id="content">
			<h1>Welcome to Machine Learning</h1>
<h1>Part 1 - Neural Networks</h1>
<script src="/node_sources/part-1/images-html/index.js"></script>
<script src="/node_sources/part-1/images-html/chart.js"></script>
<div class="important">
<p>While the most important parts are almost finished, this is still a work in
progress and I won't be able to update this post for a while.</p>
<p>I'll release the sources and notebooks once the final version is out.</p>
</div>
<p>Want to understand machine learning by looking at nice pictures instead of
boring math ? Want to learn how to use it to develop real applications in a few
lines of code ? This series is for you !</p>
<h1>Machine learning: applications</h1>
<p>Let's begin with a wikipedia definition of machine learning: (TODO)</p>
<blockquote>
<p>Machine learning is a field of computer science that gives computers the
ability to learn without being explicitly programmed.</p>
</blockquote>
<p>It allows machines to perform very complex tasks until now unreachable. Its
applications seems limitless and already produce mind-blowing results. So,
what's better to get you interested than to show you what you can do with it ?</p>
<p>TODO: css, better usages &amp; better pictures and video (robots, generative models,
, image to image, agi ?), can do/will be able to</p>
<ul>
<li>save lives by making medical diagnostics</li>
</ul>
<p><img src="/node_outputs/part-1/images/lung-cancer.jpg" alt="" width="800" height="200"> set source &amp; check copyright</p>
<ul>
<li>enrich yourself by predicting the stock market</li>
</ul>
<p><img src="/node_outputs/part-1/images/stock-market.jpg" alt="" width="800" height="200"> set source &amp; check copyright</p>
<ul>
<li>take a nap while your car drive you home</li>
</ul>
<p><img src="/node_outputs/part-1/images/self-driving-car.jpg" alt="" width="800" height="200"> set source &amp; check copyright</p>
<ul>
<li>protect the law by detecting fraud</li>
</ul>
<p><img src="/node_outputs/part-1/images/fraud-detection.jpg" alt="" width="800" height="200"> set source &amp; check copyright</p>
<ul>
<li>create your brand new bag from a doodle</li>
</ul>
<p><img src="/node_outputs/part-1/images/doodle-art.png" alt="" width="800" height="200"> set source &amp; check copyright</p>
<ul>
<li>eat your favorite meals prepared by your cooking robot chef</li>
</ul>
<p><img src="/node_outputs/part-1/images/robot-chef.jpg" alt="" width="800" height="200"> set source &amp; check copyright</p>
<ul>
<li>crush world champions at video games</li>
</ul>
<p><img src="/node_outputs/part-1/images/starcraft.jpg" alt="" width="800" height="200"> set source &amp; check copyright</p>
<p>Those are just few examples: there is no limit ! And the field is growing so
fast new applications are released every week !</p>
<p>I hope it piqued your interest and without further ado, let's see how it works !</p>
<h1>Machine learning: the big picture</h1>
<p>Machine learning is a bit like a programmer: it's used to create functions. You
begin by creating some kind of magic box: a machine learning model.</p>
<p><img src="/node_outputs/part-1/images-js/big-picture-model-only.png" alt=""></p>
<p>If you give it some data (images, texts, sounds, spreadsheets, sensor values, ...)
it gives you something in return.</p>
<p><img src="/node_outputs/part-1/images-js/big-picture-model-forward.png" alt=""></p>
<p>But at first, it has no idea what you expect it to return</p>
<p><img src="/node_outputs/part-1/images-js/big-picture-model-init.png" alt=""></p>
<p>Fortunately, it can learn. Either by itself (unsupervised learning) or by
being teached (supervised learning). Let's see the later in more details.</p>
<h1>Supervised learning</h1>
<p>Supervised learning is similar to how you would teach a child. Imagine you have
some images representing animals (cats, dogs, birds...). You show him a random
picture and ask him what animal he thinks it is.</p>
<p><img src="/node_outputs/part-1/images-js/supervised-predict.png" alt=""></p>
<p>If he got it wrong, you correct him</p>
<p><img src="/node_outputs/part-1/images-js/supervised-correct.png" alt=""></p>
<p>If he got it right, you reward him</p>
<p><img src="/node_outputs/part-1/images-js/supervised-reward.png" alt=""></p>
<p>Repeat this long enought and he will eventually learn to recognize animals.</p>
<p>The key point of supervised learning is that you must have the answers to the
questions you ask.</p>
<h1>Classification</h1>
<p>While machine learning can be used to tackle very different problems, we will
only perform classification tasks in this post. The objective of such tasks is
to predict to which categories belongs the input data.</p>
<p>An example would be lung cancer detection: given a 3d scan, you must predict if
the patient is suffering from cancer or not:</p>
<p><img src="/node_outputs/part-1/images-js/cancer-classes.png" alt=""></p>
<p>You can also have problems that have more than one categories. For instance, if
you want to know which animals are represented in a pictures, you can have one
class per kind of animals:</p>
<p><img src="/node_outputs/part-1/images-js/animal-classes.png" alt=""></p>
<h1>The project: handwritten digits recognition</h1>
<p>In this post, our objective will be to learn, step by step, how to create a
system capable of recognizing handwritten digits. The system will work as
follows: we input an image</p>
<p><img src="/node_outputs/part-1/images-js/project-input.png" alt=""></p>
<p>And the model predict which digit is represented in the image.</p>
<p><img src="/node_outputs/part-1/images-js/project-output.png" alt=""></p>
<h1>Images</h1>
<p>As we will be working with images, let's learn a bit more about them.</p>
<p>An image is composed of small colored squares, called pixels. Each pixel is
represented by a number which define its filling color. Here is an example with
huge pixels:</p>
<p><img src="/node_outputs/part-1/images-js/image-as-pixels.png" alt=""></p>
<p>In this image:</p>
<ul>
<li>a white pixel is represented by a 1</li>
<li>a black pixel is represented by a 0</li>
</ul>
<p>For the sake of keeping this post as simple as possible, we will be using images
comprised of black and white pixels only in all our examples.</p>
<p><img src="/node_outputs/part-1/images-js/pixels-01.png" alt=""></p>
<p>Now that the introduction is over, let the fun begin !</p>
<hr>
<div class="important">
<p>Before we begin, I'd like to emphasize that those posts are here to give a high
level understanding of machine learning. I tried hard to make the content
accessible to as many people as possible, but the post is long and dense. if you
feel you don't understand everything, do not give up and continue reading. Most
of the content is here to give you an intuition about how things work and is not
mandatory to understand what's follow. None of the maths presented here are
important !</p>
</div>
<h1>Detecting one white pixel</h1>
<p>Our first model will be a white pixel detector: given a pixel, we want to
classify it as white or not white. In machine learning we often represent 'yes'
as 1 and 'no' as 0.</p>
<p><img src="/node_outputs/part-1/images-js/logic-id-model.png" alt=""></p>
<p>In order to do that, we will use what's called a neural network. If you've never
seen one before, it's something that look like that:</p>
<p><img src="/node_outputs/part-1/images-js/neural-net-2332.png" alt=""></p>
<p>Those networks are made of small pieces called neurons.</p>
<p><img src="/node_outputs/part-1/images-js/neuron-neuron.png" alt=""></p>
<p>A neuron has one or more inputs. In our case, the input is only one pixel value.</p>
<p><img src="/node_outputs/part-1/images-js/neuron-input.png" alt=""></p>
<p>Each input is linked to the neuron by a weight</p>
<p><img src="/node_outputs/part-1/images-js/neuron-weight.png" alt=""></p>
<p>Remember that in our examples, there is only two pixel colors: white and black.
So there is only two possible inputs:</p>
<p><img src="/node_outputs/part-1/images-js/nnet-id-inputs.png" alt=""></p>
<p>Let's add the desired output in each case:</p>
<p><img src="/node_outputs/part-1/images-js/nnet-id-outputs.png" alt=""></p>
<p>And that's all we need to know to solve this problem ! Don't worry about the
brown neurons, we will cover them later when we will see bigger networks.</p>
<h2>Code</h2>
<p>In this post and those who follow, we will be using keras, a high level neural
network python library. It will allow us to focus on how things work instead
of worying about implementation details. You can install it using <code>pip install keras</code>.</p>
<p>First, we import the required modules and declare some useful variables.</p>
<pre><code class="language-python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np

<span class="hljs-keyword">from</span> keras.layers <span class="hljs-keyword">import</span> Dense
<span class="hljs-keyword">from</span> keras.models <span class="hljs-keyword">import</span> Sequential

BLACK = <span class="hljs-number">0</span>
WHITE = <span class="hljs-number">1</span>

<span class="hljs-comment">#&gt; Using TensorFlow backend.</span>
</code></pre>
<p>Then we create our model and our neuron.</p>
<pre><code class="language-python"><span class="hljs-comment"># create a new model</span>
pixel_is_white_model = Sequential()
<span class="hljs-comment"># create a single neuron with a single input and add it to our model</span>
pixel_is_white_model.add(Dense(<span class="hljs-number">1</span>, use_bias = <span class="hljs-keyword">False</span>, input_shape = (<span class="hljs-number">1</span>,)))
</code></pre>
<p>As we can see, a neuron is created with <code>Dense</code>. The first argument is the
number of neurons we want to create and <code>input_shape</code> is the number of inputs.<br>
Don't worry about <code>Sequential</code> and <code>use_bias</code>, we will talk about them later.</p>
<p>Now that our model is ready, we need to set up our training. As described in the
introduction, we will use a supervised learning technique: we will show our
model different pixel values and correct/reward it depending on its answers.
To do that we need to prepare a dataset comprised of pixel values and their
expected outputs (called labels).</p>
<pre><code class="language-python">pixel_is_white_inputs = [BLACK, WHITE]
pixel_is_white_labels = [<span class="hljs-number">0</span>,     <span class="hljs-number">1</span>]
</code></pre>
<p>Then we can train our model.</p>
<pre><code class="language-python"><span class="hljs-comment"># disregard this function for now, it will be explained later</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">train</span><span class="hljs-params">(model, x, y)</span>:</span>
    model.compile(
        optimizer = <span class="hljs-string">"sgd"</span>,
        loss      = <span class="hljs-string">"mean_squared_error"</span>,
        metrics   = [<span class="hljs-string">"accuracy"</span>]
    )
    model.fit(x, y, epochs = <span class="hljs-number">1000</span>)

<span class="hljs-comment"># show our pixels to our model and correct/reward it</span>
train(
    pixel_is_white_model,
    pixel_is_white_inputs,
    pixel_is_white_labels,
)

<span class="hljs-comment">#&gt; ...</span>
<span class="hljs-comment">#&gt; 2/2 [==============================] - 0s - loss: 1.0960e-07 - acc: 1.0000</span>
<span class="hljs-comment">#&gt; Epoch 1999/2000</span>
<span class="hljs-comment">#&gt; 2/2 [==============================] - 0s - loss: 1.0960e-07 - acc: 1.0000</span>
<span class="hljs-comment">#&gt; Epoch 2000/2000</span>
<span class="hljs-comment">#&gt; 2/2 [==============================] - 0s - loss: 1.0960e-07 - acc: 1.0000</span>
</code></pre>
<div class="note">
<p>Don't pay too much attention about the output. We just want to make sure that
'acc', short for accuracy, is equal to 1.0000 (i.e our model predict the good
value 100% of the time).<br>
If not, run <code>train()</code> until you get this result.</p>
</div>
<p>And that's it ! We have a trained model ready to be used ! Let's use the
<code>predict_classes</code> method to get our model prediction. For each input, it returns
the predicted class: 1 for white and 0 otherwise.</p>
<pre><code class="language-python">pixel_is_white_model.predict_classes(np.array([BLACK]))

<span class="hljs-comment">#&gt; array([[0]], dtype=int32)</span>

pixel_is_white_model.predict_classes(np.array([WHITE]))

<span class="hljs-comment">#&gt; array([[1]], dtype=int32)</span>

pixel_is_white_model.predict_classes(pixel_is_white_inputs)

<span class="hljs-comment">#&gt; array([[0],</span>
<span class="hljs-comment">#&gt;        [1]], dtype=int32)</span>
</code></pre>
<p>Nice, it predicted exactly what we wanted ! And you may not believe it, but this
will be almost the whole code we need for handwritten digits recognition !</p>
<h1>Intuition and experiments</h1>
<p>How does this compute anything at all ? Here is the trick: each weight has a
value.</p>
<p><img src="/node_outputs/part-1/images-js/nnet-id-weight.png" alt=""></p>
<p>A neuron value is equal to the input times the weight:</p>
<p><img src="/node_outputs/part-1/images-js/nnet-id-equation.png" alt=""></p>
<p>If we input a white pixel, the equation become:</p>
<p><img src="/node_outputs/part-1/images-js/nnet-id-input-white.png" alt=""></p>
<p>And we know that for a white pixel, the desired output is 1.</p>
<p><img src="/node_outputs/part-1/images-js/nnet-id-output-white.png" alt=""></p>
<p>So we need to find <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>w</mi></mrow><annotation encoding="application/x-tex">w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.02691em;">w</span></span></span></span>, the weight value, that solve this equation. But
remember, we don't have only one possible input: the weight must work for each
case.</p>
<p><img src="/node_outputs/part-1/images-js/nnet-id-equations.png" alt=""></p>
<p>Here, we can find the solution by ourselves: <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>w</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">w = 1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.64444em;"></span><span class="strut bottom" style="height:0.64444em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.02691em;">w</span><span class="mrel">=</span><span class="mord mathrm">1</span></span></span></span></p>
<p><img src="/node_outputs/part-1/images-js/nnet-id-solutions.png" alt=""></p>
<p>And this is exactly what the library is doing for us ! No matter how complex
your network is, training is just a matter of finding good weights.</p>
<p>Let's experiment with what we learned. Here is an equivalent of our current
network in plain python:</p>
<pre><code class="language-python">weight = <span class="hljs-number">1</span>

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">neuron</span><span class="hljs-params">(input_pixel)</span>:</span>
    <span class="hljs-keyword">return</span> input_pixel * weight

neuron(BLACK)

<span class="hljs-comment">#&gt; 0</span>

neuron(WHITE)

<span class="hljs-comment">#&gt; 1</span>
</code></pre>
<p>As we will see later, it's almost impossible for our library to find exactly the
best weight's value. Even in this simple case it did not find exactly 1:</p>
<pre><code class="language-python"><span class="hljs-comment"># do not think too much about this line, it just returns the model's weight</span>
weight_model = pixel_is_white_model.get_weights()[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>][<span class="hljs-number">0</span>]
weight_model

<span class="hljs-comment">#&gt; 0.99861383</span>
</code></pre>
<p>If we wanted to get the real output, we should have used the <code>predict</code> method.</p>
<pre><code class="language-python">pixel_is_white_model.predict(np.array([BLACK]))

<span class="hljs-comment">#&gt; array([[ 0.]], dtype=float32)</span>

pixel_is_white_model.predict(np.array([WHITE]))

<span class="hljs-comment">#&gt; array([[ 0.99861383]], dtype=float32)</span>
</code></pre>
<p>We can now update our previous implementation to use the model's weight:</p>
<pre><code class="language-python">weight = weight_model

neuron(BLACK)

<span class="hljs-comment">#&gt; 0.0</span>

neuron(WHITE)

<span class="hljs-comment">#&gt; 0.99861383438110352</span>
</code></pre>
<p>And we get the exact same results as our network !</p>
<p>For those who are curious, <code>predict_classes</code> is equivalent to (for one output
only):</p>
<pre><code class="language-python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">predict_classes</span><span class="hljs-params">(output)</span>:</span>
    <span class="hljs-keyword">if</span> output &gt; <span class="hljs-number">0.5</span>:
        <span class="hljs-keyword">return</span> <span class="hljs-number">1</span>
    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>

predict_classes(neuron(WHITE))

<span class="hljs-comment">#&gt; 1</span>
</code></pre>
<h1>Detecting two white pixels</h1>
<p>Let's do the same thing, but with more inputs this time. Given two pixels, our model should return:</p>
<ul>
<li>1 if both are white</li>
<li>0 otherwise</li>
</ul>
<p>We need to use a network with two inputs:</p>
<p><img src="/node_outputs/part-1/images-js/nnet-and-nnet.png" alt=""></p>
<p>There is almost no change in the code, we just have to update <code>input_shape</code> and
replace the inputs and labels. (TODO: diff highlight)</p>
<pre><code class="language-python">both_are_white_model = Sequential()
both_are_white_model.add(Dense(<span class="hljs-number">1</span>, use_bias = <span class="hljs-keyword">False</span>, input_shape = (<span class="hljs-number">2</span>,)))

both_are_white_inputs = [[BLACK, BLACK], [WHITE, BLACK], [WHITE, WHITE], [BLACK, WHITE]]
both_are_white_labels = [<span class="hljs-number">0</span>,              <span class="hljs-number">0</span>,              <span class="hljs-number">1</span>,              <span class="hljs-number">0</span>]

train(
    both_are_white_model,
    both_are_white_inputs,
    both_are_white_labels
)

<span class="hljs-comment">#&gt; ...</span>
<span class="hljs-comment">#&gt; Epoch 1999/2000</span>
<span class="hljs-comment">#&gt; 4/4 [==============================] - 0s - loss: 0.0833 - acc: 1.0000</span>
<span class="hljs-comment">#&gt; Epoch 2000/2000</span>
<span class="hljs-comment">#&gt; 4/4 [==============================] - 0s - loss: 0.0833 - acc: 1.0000</span>

both_are_white_model.predict_classes(both_are_white_inputs)

<span class="hljs-comment">#&gt; array([[0],</span>
<span class="hljs-comment">#&gt;        [0],</span>
<span class="hljs-comment">#&gt;        [1],</span>
<span class="hljs-comment">#&gt;        [0]], dtype=int32)</span>

both_are_white_model.predict(both_are_white_inputs)

<span class="hljs-comment">#&gt; array([[ 0.        ],</span>
<span class="hljs-comment">#&gt;        [ 0.33332399],</span>
<span class="hljs-comment">#&gt;        [ 0.66666663],</span>
<span class="hljs-comment">#&gt;        [ 0.33334267]], dtype=float32)</span>
</code></pre>
<p>Nothing new, so let's look at the maths !</p>
<h1>Intuition and experiments</h1>
<p>When a neuron has multiple inputs, it just sum their products:</p>
<p><img src="/node_outputs/part-1/images-js/nnet-and-equation.png" alt=""></p>
<p>Here is the associated computation graph:</p>
<p><img src="/node_outputs/part-1/images-js/nnet-and-equation-nnet.png" alt=""></p>
<p>Let's update our python implementation:</p>
<pre><code class="language-python">w1, w2 = both_are_white_model.get_weights()[<span class="hljs-number">0</span>]
w1

<span class="hljs-comment">#&gt; array([ 0.33332399], dtype=float32)</span>

w2

<span class="hljs-comment">#&gt; array([ 0.33334267], dtype=float32)</span>

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">neuron</span><span class="hljs-params">(x1, x2)</span>:</span>
    y  = x1 * w1
    y += x2 * w2
    <span class="hljs-keyword">return</span> y

neuron(BLACK, WHITE)

<span class="hljs-comment">#&gt; array([ 0.33334267], dtype=float32)</span>

neuron(WHITE, WHITE)

<span class="hljs-comment">#&gt; array([ 0.66666663], dtype=float32)</span>
</code></pre>
<p>Once again, we get the same results as our network.</p>
<h1>Both not black model</h1>
<p>Let's do a last example to familiarize a bit more with keras: given two pixels,
outputs:</p>
<ul>
<li>0 if both pixels are black</li>
<li>1 otherwise</li>
</ul>
<p>This is exactly the same problem as before, just with different labels. So let's
use this example to introduce a bit of notation:</p>
<ul>
<li>inputs are often associated with the variable 'x'</li>
<li>labels are often associated with the variable 'y'.</li>
</ul>
<pre><code class="language-python">both_not_black_model = Sequential()
both_not_black_model.add(Dense(<span class="hljs-number">1</span>, use_bias = <span class="hljs-keyword">False</span>, input_shape = (<span class="hljs-number">2</span>,)))

<span class="hljs-comment"># inputs are often referred as 'x'</span>
<span class="hljs-comment"># labels are often referred as 'y'</span>
both_not_black_x = [[BLACK, BLACK], [WHITE, BLACK], [WHITE, WHITE], [BLACK, WHITE]]
both_not_black_y = [<span class="hljs-number">0</span>,              <span class="hljs-number">1</span>,              <span class="hljs-number">1</span>,              <span class="hljs-number">1</span>]

train(
    both_not_black_model,
    both_not_black_x,
    both_not_black_y
)

<span class="hljs-comment">#&gt; ...</span>
<span class="hljs-comment">#&gt; Epoch 1999/2000</span>
<span class="hljs-comment">#&gt; 4/4 [==============================] - 0s - loss: 0.0833 - acc: 1.0000</span>
<span class="hljs-comment">#&gt; Epoch 2000/2000</span>
<span class="hljs-comment">#&gt; 4/4 [==============================] - 0s - loss: 0.0833 - acc: 1.0000</span>

both_not_black_model.predict_classes(both_not_black_x)

<span class="hljs-comment">#&gt; array([[0],</span>
<span class="hljs-comment">#&gt;        [1],</span>
<span class="hljs-comment">#&gt;        [1],</span>
<span class="hljs-comment">#&gt;        [1]], dtype=int32)</span>

both_not_black_model.predict(both_not_black_x)

<span class="hljs-comment">#&gt; array([[ 0.        ],</span>
<span class="hljs-comment">#&gt;        [ 0.6666919 ],</span>
<span class="hljs-comment">#&gt;        [ 1.33333325],</span>
<span class="hljs-comment">#&gt;        [ 0.66664141]], dtype=float32)</span>
</code></pre>
<p>Those small networks should have no more secret for you.</p>
<h1>Generic code</h1>
<p>Until now, we wrote a specific <code>neuron()</code> function for each network, let's
implement a generic version.</p>
<p>We pass the inputs and weights as lists:</p>
<pre><code class="language-python">x = [x1, x2]
w = [w1, w2]
</code></pre>
<p>Then we sum the product of each pair:</p>
<pre><code class="language-python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">neuron</span><span class="hljs-params">(x, w)</span>:</span>
    y = <span class="hljs-number">0</span>
    n = len(x)
    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(n):
        y += x[i] * w[i]
    <span class="hljs-keyword">return</span> y

neuron(x, w)
</code></pre>
<p>TODO: notation in real life &amp; dot product</p>
<h1>Pixel is black model</h1>
<p>Now that we are confortable with small networks, let's look at one of their
weaknesses.</p>
<p>Let's create a model that classify an input pixel as black or not black:</p>
<ul>
<li>1 if the pixel is black</li>
<li>0 otherwise</li>
</ul>
<p>This problem seems identical to the previous ones, but there is a subtlety that
require our attention:</p>
<p><img src="/node_outputs/part-1/images-js/nnet-not-equations.png" alt=""></p>
<p>We can see that if we input a black pixel, the weight is multiplied by 0. No
matter its value, the result will always be 0.</p>
<p>Fortunately there is an easy fix: biases.</p>
<h1>Biases</h1>
<p>A bias is just another neuron we add to the inputs.</p>
<p><img src="/node_outputs/part-1/images-js/nnet-not-bias.png" alt=""></p>
<p>It has a fixed value of 1.</p>
<p><img src="/node_outputs/part-1/images-js/nnet-not-bias-value.png" alt=""></p>
<p>And like any other neuron, it has a weight.</p>
<p><img src="/node_outputs/part-1/images-js/nnet-not-bias-weight.png" alt=""></p>
<p>The equation is the same as a network with two inputs:</p>
<p><img src="/node_outputs/part-1/images-js/nnet-not-bias-equation.png" alt=""></p>
<p>We can see this simple little trick solve our problem:</p>
<p><img src="/node_outputs/part-1/images-js/nnet-not-bias-solutions.png" alt=""></p>
<h1>Code</h1>
<p>Remember the <code>use_bias</code> parameter we didn't talk about ? It does exactly what
its name implies. Using a bias is almost always useful, so we will set it to
<code>True</code>, its default value.</p>
<pre><code class="language-python">pixel_is_black_m = Sequential()
pixel_is_black_m.add(Dense(<span class="hljs-number">1</span>, use_bias = <span class="hljs-keyword">True</span>, input_shape = (<span class="hljs-number">1</span>,)))

pixel_is_black_x = [BLACK, WHITE]
pixel_is_black_y = [<span class="hljs-number">1</span>,     <span class="hljs-number">0</span>]

train(
    pixel_is_black_m,
    pixel_is_black_x,
    pixel_is_black_y
)

pixel_is_black_m.predict_classes(pixel_is_black_x)

<span class="hljs-comment">#&gt; array([[1],</span>
<span class="hljs-comment">#&gt;        [0]], dtype=int32)</span>

pixel_is_black_m.predict(pixel_is_black_x)

<span class="hljs-comment">#&gt; array([[ 0.97386539],</span>
<span class="hljs-comment">#&gt;        [ 0.00688618]], dtype=float32)</span>
</code></pre>
<p>Once again, let's update our implementation:</p>
<pre><code class="language-python">[w], b = pixel_is_black_m.get_weights()
w

<span class="hljs-comment">#&gt; array([-0.96697921], dtype=float32)</span>

b

<span class="hljs-comment">#&gt; array([ 0.97386539], dtype=float32)</span>

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">neuron</span><span class="hljs-params">(x)</span>:</span>
    y  = <span class="hljs-number">1</span> * b
    y += x * w
    <span class="hljs-keyword">return</span> y

neuron(BLACK)

<span class="hljs-comment">#&gt; array([[ 0.97386539]], dtype=float32)</span>

neuron(WHITE)

<span class="hljs-comment">#&gt; array([[ 0.00688618]], dtype=float32)</span>
</code></pre>
<p>With the generic version, a bias is just an input with a fixed value of 1 and a
corresponding weight:</p>
<pre><code class="language-python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">neuron</span><span class="hljs-params">(x, w)</span>:</span>
    y = <span class="hljs-number">0</span>
    n = len(x)
    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(n):
        y += x[i] * w[i]
    <span class="hljs-keyword">return</span> y

x = [<span class="hljs-number">1</span>, BLACK]
w = [b, w]

neuron(x, w)

<span class="hljs-comment">#&gt; array([[ 0.97386539]], dtype=float32)</span>
</code></pre>
<h1>More about neural networks</h1>
<p>Before we look at a more serious issues, we need to learn a bit more about
neural networks and keras.</p>
<p>Let's take a fresh look at the first neural network picture:</p>
<p><img src="/node_outputs/part-1/images-js/neural-net-2332.png" alt=""></p>
<p>We can see that neurons form groups called layers.</p>
<p><img src="/node_outputs/part-1/images-js/neural-net-2332-layers.png" alt=""></p>
<p>Layers have special names:</p>
<ul>
<li>The first one is called input layer</li>
<li>The last one is called output layer</li>
<li>All the others, in between, are called hidden layers</li>
</ul>
<p><img src="/node_outputs/part-1/images-js/neural-net-2332-layers-names.png" alt=""></p>
<p>The weights belongs to the layers, not the inputs: (rephrase)</p>
<p><img src="/node_outputs/part-1/images-js/neural-net-2332-weights-names.png" alt=""></p>
<p>We are now ready to talk about <code>Dense</code> and <code>Sequential</code> !</p>
<h2>Dense</h2>
<p>If we look at the imports, we can see that <code>Dense</code> is a layer.</p>
<pre><code class="language-python"><span class="hljs-keyword">from</span> keras.layers <span class="hljs-keyword">import</span> Dense
</code></pre>
<p>A <code>Dense</code> layer simply create a bunch of neurons and connect each one of them to
every single input.</p>
<pre><code class="language-python">Dense(<span class="hljs-number">3</span>, use_bias = <span class="hljs-keyword">False</span>, input_shape = (<span class="hljs-number">2</span>,))
</code></pre>
<p><img src="/node_outputs/part-1/images-js/neural-net-23.png" alt=""></p>
<pre><code class="language-python">Dense(<span class="hljs-number">3</span>, use_bias = <span class="hljs-keyword">True</span>, input_shape = (<span class="hljs-number">2</span>,))
</code></pre>
<p><img src="/node_outputs/part-1/images-js/neural-net-23-bias.png" alt=""></p>
<p><code>Dense</code> layers are often referred as Fully Connected (FC) layers in the
litterature.</p>
<div class="note">
<p>There is more than one kind of layer, we will introduce them as we go.</p>
</div>
<h2>Sequential</h2>
<p>Until now, we only created networks with one layer. What if wanted we create a
deeper network ? Well, this is where <code>Sequential</code> kicks in.</p>
<pre><code class="language-python"><span class="hljs-keyword">from</span> keras.models <span class="hljs-keyword">import</span> Sequential
</code></pre>
<p><code>Sequential</code> is a model that stacks layers: every time you <code>add()</code> a layer, it is
inserted at the end of the model.</p>
<pre><code class="language-python">model = Sequential()
model.add(Dense(<span class="hljs-number">3</span>, use_bias = <span class="hljs-keyword">False</span>, input_shape = (<span class="hljs-number">2</span>,)))
</code></pre>
<p><img src="/node_outputs/part-1/images-js/neural-net-23.png" alt=""></p>
<pre><code class="language-python">model.add(Dense(<span class="hljs-number">3</span>, use_bias = <span class="hljs-keyword">False</span>))
</code></pre>
<p><img src="/node_outputs/part-1/images-js/neural-net-233.png" alt=""></p>
<pre><code class="language-python">model.add(Dense(<span class="hljs-number">2</span>, use_bias = <span class="hljs-keyword">False</span>))
</code></pre>
<p><img src="/node_outputs/part-1/images-js/neural-net-2332.png" alt=""></p>
<p>You may have noticed that only the first layer require <code>input_shape</code>. All
subsequent layers will use the neurons of the previous layer as inputs.</p>
<h2>Playing with keras</h2>
<p>Keras offers some nice methods and attributes to play with layers. I will
briefly introduce those we will use in this post. Refer to the official
documentation for more information.</p>
<ul>
<li><code>Model.summary()</code>: displays a textual represention of the model</li>
</ul>
<pre><code class="language-python">model.summary()

<span class="hljs-comment">#&gt; _________________________________________________________________</span>
<span class="hljs-comment">#&gt; Layer (type)                 Output Shape              Param #</span>
<span class="hljs-comment">#&gt; =================================================================</span>
<span class="hljs-comment">#&gt; dense_1 (Dense)              (None, 3)                 6</span>
<span class="hljs-comment">#&gt; _________________________________________________________________</span>
<span class="hljs-comment">#&gt; dense_2 (Dense)              (None, 3)                 9</span>
<span class="hljs-comment">#&gt; _________________________________________________________________</span>
<span class="hljs-comment">#&gt; dense_3 (Dense)              (None, 2)                 6</span>
<span class="hljs-comment">#&gt; =================================================================</span>
<span class="hljs-comment">#&gt; Total params: 21</span>
<span class="hljs-comment">#&gt; Trainable params: 21</span>
<span class="hljs-comment">#&gt; Non-trainable params: 0</span>
<span class="hljs-comment">#&gt; _________________________________________________________________</span>
</code></pre>
<ul>
<li><code>Model.layers</code>: attribute containing all the layers added to the model</li>
</ul>
<pre><code class="language-python">model.layers

<span class="hljs-comment">#&gt; [&lt;keras.layers.core.Dense at 0x7f3797d52cc0&gt;,</span>
<span class="hljs-comment">#&gt;  &lt;keras.layers.core.Dense at 0x7f3797d31940&gt;,</span>
<span class="hljs-comment">#&gt;  &lt;keras.layers.core.Dense at 0x7f3797cd4c18&gt;]</span>
</code></pre>
<ul>
<li><code>Model.get_weights()</code>: get the weights of each layer</li>
<li><code>Model.set_weights()</code>: set the weights of each layer</li>
</ul>
<pre><code class="language-python">model.get_weights()

<span class="hljs-comment">#&gt; [array([[-0.71340144, -0.03733754,  0.17574036],</span>
<span class="hljs-comment">#&gt;         [-0.93694097, -0.81433576, -0.4139728 ]], dtype=float32),</span>
<span class="hljs-comment">#&gt;  array([[-0.64885306, -0.59295535,  0.0623827 ],</span>
<span class="hljs-comment">#&gt;         [-0.86857986, -0.84055448,  0.59113455],</span>
<span class="hljs-comment">#&gt;         [-0.59811687,  0.80146456, -0.72720838]], dtype=float32),</span>
<span class="hljs-comment">#&gt;  array([[-1.04037547, -0.68300182],</span>
<span class="hljs-comment">#&gt;         [ 0.88082504,  0.62239075],</span>
<span class="hljs-comment">#&gt;         [ 0.56199396,  0.19530547]], dtype=float32)]</span>

model.set_weights([
    [[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">2</span>)], <span class="hljs-comment"># 3 neurons, 2 inputs</span>
    [[<span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">3</span>)], <span class="hljs-comment"># 3 neurons, 3 inputs</span>
    [[<span class="hljs-number">7</span>, <span class="hljs-number">8</span>   ] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">3</span>)], <span class="hljs-comment"># 2 neurons, 3 inputs</span>
])

model.get_weights()

<span class="hljs-comment">#&gt; [array([[ 1.,  2.,  3.],</span>
<span class="hljs-comment">#&gt;         [ 1.,  2.,  3.]], dtype=float32),</span>
<span class="hljs-comment">#&gt;  array([[ 4.,  5.,  6.],</span>
<span class="hljs-comment">#&gt;         [ 4.,  5.,  6.],</span>
<span class="hljs-comment">#&gt;         [ 4.,  5.,  6.]], dtype=float32),</span>
<span class="hljs-comment">#&gt;  array([[ 7.,  8.     ],</span>
<span class="hljs-comment">#&gt;         [ 7.,  8.     ],</span>
<span class="hljs-comment">#&gt;         [ 7.,  8.     ]], dtype=float32)]</span>
</code></pre>
<ul>
<li><code>Layer.get_weights()</code>: get the weights of a layer</li>
<li><code>Layer.set_weights()</code>: set the weights of a layer</li>
</ul>
<pre><code class="language-python">model.layers[<span class="hljs-number">0</span>].get_weights()

<span class="hljs-comment">#&gt; [array([[ 1.,  2.,  3.],</span>
<span class="hljs-comment">#&gt;         [ 1.,  2.,  3.]], dtype=float32)]</span>

model.layers[<span class="hljs-number">1</span>].get_weights()

<span class="hljs-comment">#&gt; [array([[ 4.,  5.,  6.],</span>
<span class="hljs-comment">#&gt;         [ 4.,  5.,  6.],</span>
<span class="hljs-comment">#&gt;         [ 4.,  5.,  6.]], dtype=float32)]</span>

model.layers[<span class="hljs-number">2</span>].get_weights()
model.layers[<span class="hljs-number">2</span>].set_weights([np.array([
    [<span class="hljs-number">7</span>, <span class="hljs-number">8</span>],
    [<span class="hljs-number">7</span>, <span class="hljs-number">8</span>],
    [<span class="hljs-number">7</span>, <span class="hljs-number">8</span>],
])])

<span class="hljs-comment">#&gt; [array([[ 7.,  8.],</span>
<span class="hljs-comment">#&gt;         [ 7.,  8.],</span>
<span class="hljs-comment">#&gt;         [ 7.,  8.]], dtype=float32)]</span>
</code></pre>
<h3>Math</h3>
<p>How do we compute something when we have multiple neurons ? We evaluate each one
independently. They all have their own weights and share the same inputs.</p>
<style>
.carousel > p
{
    display: none;
}
</style>
<div id="layer-computation" class="carousel">
<p><img src="/node_outputs/part-1/images-js/nnet-23-inputs.png" alt="">
<img src="/node_outputs/part-1/images-js/nnet-23-neuron-1.png" alt="">
<img src="/node_outputs/part-1/images-js/nnet-23-neuron-2.png" alt="">
<img src="/node_outputs/part-1/images-js/nnet-23-neuron-3.png" alt="">
<img src="/node_outputs/part-1/images-js/nnet-23-outputs.png" alt=""></p>
</div>
<script> createCarousel($("#layer-computation p")) </script>
<p>And when we have multiple layers ? Well we begin by computing the first layer
using the input data. Then we compute the next layer using the
previous layer as input, until the end.</p>
<div id="network-computation" class="carousel">
<p><img src="/node_outputs/part-1/images-js/nnet-2332-inputs.png" alt="">
<img src="/node_outputs/part-1/images-js/nnet-2332-layer-x-0.png" alt="">
<img src="/node_outputs/part-1/images-js/nnet-2332-layer-z-1.png" alt="">
<img src="/node_outputs/part-1/images-js/nnet-2332-layer-x-1.png" alt="">
<img src="/node_outputs/part-1/images-js/nnet-2332-layer-z-2.png" alt="">
<img src="/node_outputs/part-1/images-js/nnet-2332-layer-x-2.png" alt="">
<img src="/node_outputs/part-1/images-js/nnet-2332-layer-z-3.png" alt="">
<img src="/node_outputs/part-1/images-js/nnet-2332-layer-x-3.png" alt="">
<img src="/node_outputs/part-1/images-js/nnet-2332-outputs.png" alt=""></p>
</div>
<script> createCarousel($("#network-computation p")) </script>
<p>As we can see we start with inputs on the left and we go all the way to the
right. This is called the forward pass:</p>
<p><img src="/node_outputs/part-1/images-js/nnet-2332-forward.png" alt=""></p>
<h1>Code</h1>
<p>a <code>Dense</code> layer keep tracks of each neuron's weights. When fed with some inputs,
it will return the computed output of each neuron.</p>
<pre><code class="language-python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">DenseLayer</span><span class="hljs-params">(object)</span>:</span>

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, nb_neurons, input_shape)</span>:</span>
        <span class="hljs-string">"""
        For each neuron:
            create `input_shape` random weights: one for each input
        """</span>
        self.neurons_weights = []
        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(nb_neurons):
            self.neurons_weights.append(
                np.random.random(input_shape)
            )

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">predict</span><span class="hljs-params">(self, inputs)</span>:</span>
        <span class="hljs-string">"""
        For each neuron:
            compute its output
        returns all outputs
        """</span>
        outputs = []
        <span class="hljs-keyword">for</span> neuron_weights <span class="hljs-keyword">in</span> self.neurons_weights:
            outputs.append(neuron(inputs, neuron_weights))
        <span class="hljs-keyword">return</span> outputs
</code></pre>
<p><code>Sequential</code> is rather straightforward:</p>
<pre><code class="language-python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">SequentialModel</span><span class="hljs-params">(object)</span>:</span>

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self)</span>:</span>
        self.layers = []

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">add</span><span class="hljs-params">(self, layer)</span>:</span>
        self.layers.append(layer)

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">predict</span><span class="hljs-params">(self, inputs)</span>:</span>
        <span class="hljs-string">"""
        For each layer:
            compute its output using the previous layer's output as input
        returns last layer's output
        """</span>
        <span class="hljs-keyword">for</span> layer <span class="hljs-keyword">in</span> self.layers:
            inputs = layer.predict(inputs)
        <span class="hljs-keyword">return</span> inputs
</code></pre>
<p>Finally, we can use all those pieces to create a multi-layer network:</p>
<pre><code class="language-python">model = SequentialModel()
model.add(DenseLayer(<span class="hljs-number">3</span>, input_shape = <span class="hljs-number">2</span>))
model.add(DenseLayer(<span class="hljs-number">3</span>, input_shape = <span class="hljs-number">3</span>))
model.add(DenseLayer(<span class="hljs-number">2</span>, input_shape = <span class="hljs-number">3</span>))

model.predict([<span class="hljs-number">2</span>, <span class="hljs-number">3</span>])
</code></pre>
<div class="note">
<p>Whether you want to reimplement part of the api or you have troubles
understanding a certain topic, I highly encourage you to take a look at keras
source code. It's well written and well documented, it should be a great source
of knowledges.</p>
</div>
<h1>Current limitations of our models</h1>
<p>Now that we learned how to build networks with multiple layers and neurons, it's
time to see why they are better than our simple one neuron models... or not.</p>
<p>Neural networks are just big mathematical functions: a bunch of additions and
multiplications. Each time we change the weights, our model represent a
different function.</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>y</mi><mo>=</mo><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">y = model(x)
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base displaystyle textstyle uncramped"><span class="mord mathit" style="margin-right:0.03588em;">y</span><span class="mrel">=</span><span class="mord mathit">m</span><span class="mord mathit">o</span><span class="mord mathit">d</span><span class="mord mathit">e</span><span class="mord mathit" style="margin-right:0.01968em;">l</span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mclose">)</span></span></span></span></span></p>
<p>Thus we can plot, for each value of x, the conresponding output.</p>
<p>Let's take back our <code>pixel_is_white</code> model:</p>
<ul>
<li>if the input pixel is white (1), the model return 1</li>
<li>if the input pixel is black (0), the model return 0</li>
</ul>
<p>Here is a graph representing this problem: (TODO: explain school vs non school ?)</p>
<p><img src="/node_outputs/part-1/images-js/graph-id-dataset.png" alt=""></p>
<p>The <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">x</span></span></span></span> axis represent the value taken by the input pixel, and the <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.03588em;">y</span></span></span></span> axis
represent the output value.</p>
<p>Any function that pass by those two points would be a solution to our problem
because we only need the following to be true:</p>
<ul>
<li><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi><mo>(</mo><mi>b</mi><mi>l</mi><mi>a</mi><mi>c</mi><mi>k</mi><mo>)</mo><mo>=</mo><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi><mo>(</mo><mn>0</mn><mo>)</mo><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">model(black) = model(0) = 0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord mathit">m</span><span class="mord mathit">o</span><span class="mord mathit">d</span><span class="mord mathit">e</span><span class="mord mathit" style="margin-right:0.01968em;">l</span><span class="mopen">(</span><span class="mord mathit">b</span><span class="mord mathit" style="margin-right:0.01968em;">l</span><span class="mord mathit">a</span><span class="mord mathit">c</span><span class="mord mathit" style="margin-right:0.03148em;">k</span><span class="mclose">)</span><span class="mrel">=</span><span class="mord mathit">m</span><span class="mord mathit">o</span><span class="mord mathit">d</span><span class="mord mathit">e</span><span class="mord mathit" style="margin-right:0.01968em;">l</span><span class="mopen">(</span><span class="mord mathrm">0</span><span class="mclose">)</span><span class="mrel">=</span><span class="mord mathrm">0</span></span></span></span></li>
<li><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi><mo>(</mo><mi>w</mi><mi>h</mi><mi>i</mi><mi>t</mi><mi>e</mi><mo>)</mo><mo>=</mo><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi><mo>(</mo><mn>1</mn><mo>)</mo><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">model(white) = model(1) = 1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord mathit">m</span><span class="mord mathit">o</span><span class="mord mathit">d</span><span class="mord mathit">e</span><span class="mord mathit" style="margin-right:0.01968em;">l</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.02691em;">w</span><span class="mord mathit">h</span><span class="mord mathit">i</span><span class="mord mathit">t</span><span class="mord mathit">e</span><span class="mclose">)</span><span class="mrel">=</span><span class="mord mathit">m</span><span class="mord mathit">o</span><span class="mord mathit">d</span><span class="mord mathit">e</span><span class="mord mathit" style="margin-right:0.01968em;">l</span><span class="mopen">(</span><span class="mord mathrm">1</span><span class="mclose">)</span><span class="mrel">=</span><span class="mord mathrm">1</span></span></span></span></li>
</ul>
<p><img src="/node_outputs/part-1/images-js/graph-id-solutions.png" alt=""></p>
<p>Now, remember the equation of our network:</p>
<p><img src="/node_outputs/part-1/images-js/nnet-id-equation.png" alt=""></p>
<p>We found that <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>w</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">w = 1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.64444em;"></span><span class="strut bottom" style="height:0.64444em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.02691em;">w</span><span class="mrel">=</span><span class="mord mathrm">1</span></span></span></span> was the solution.</p>
<p><img src="/node_outputs/part-1/images-js/graph-id-solution.png" alt=""></p>
<p>Let's try other weight values to see what the generated functions looks like:</p>
<p><img src="/node_outputs/part-1/images-js/graph-id-weights.png" alt=""></p>
<p>We can see that no matter the value of the weight, we observe two things:</p>
<ul>
<li>the function is always a line</li>
<li>the function always returns 0 when <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>x</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">x = 0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.64444em;"></span><span class="strut bottom" style="height:0.64444em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">x</span><span class="mrel">=</span><span class="mord mathrm">0</span></span></span></span></li>
</ul>
<p>The second observation is what bothered us with <code>pixel_is_black</code>: if we input a
black pixel (0), no matter the weight's value, the result will always be 0.</p>
<p><img src="/node_outputs/part-1/images-js/nnet-not-equations.png" alt=""></p>
<p>If we had plotted the dataset, we could have spotted the problem without looking
at the equations.</p>
<ul>
<li>if the input pixel is black (0), the function returns 1</li>
<li>if the input pixel is white (1), the function returns 0</li>
</ul>
<p><img src="/node_outputs/part-1/images-js/graph-not-dataset.png" alt=""></p>
<p>We can see that if we only create functions that return 0 when <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>x</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">x = 0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.64444em;"></span><span class="strut bottom" style="height:0.64444em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">x</span><span class="mrel">=</span><span class="mord mathrm">0</span></span></span></span>, we cannot
pass by the first point.</p>
<p>But then, we introduced biases:</p>
<p><img src="/node_outputs/part-1/images-js/nnet-not-bias-equation.png" alt=""></p>
<p>And we found that <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>w</mi><mo>=</mo><mo>−</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">w = -1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.64444em;"></span><span class="strut bottom" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.02691em;">w</span><span class="mrel">=</span><span class="mord">−</span><span class="mord mathrm">1</span></span></span></span> and <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>b</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">b = 1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.69444em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">b</span><span class="mrel">=</span><span class="mord mathrm">1</span></span></span></span> worked</p>
<p><img src="/node_outputs/part-1/images-js/graph-not-solution.png" alt=""></p>
<p>The bias allows us to create functions that return something other than 0 when
<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>x</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">x = 0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.64444em;"></span><span class="strut bottom" style="height:0.64444em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">x</span><span class="mrel">=</span><span class="mord mathrm">0</span></span></span></span>. Here are some more examples with <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>w</mi><mo>=</mo><mo>−</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">w = -1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.64444em;"></span><span class="strut bottom" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.02691em;">w</span><span class="mrel">=</span><span class="mord">−</span><span class="mord mathrm">1</span></span></span></span> but different bias
values:</p>
<p><img src="/node_outputs/part-1/images-js/graph-not-biases.png" alt=""></p>
<p>While the bias allows us to move the line in any direction, we are still limited
to produce 'line' functions, called linear functions in the mathematical world.</p>
<p>Why is that bothering us ? Let's imagine we add one new purple pixel represented
by the number 2:</p>
<p><img src="/node_outputs/part-1/images-js/pixels-012.png" alt=""></p>
<p>Here is the new graph of <code>pixel_is_white</code>. The only change is that we must also
return 0 for purple, because... it's not white</p>
<p><img src="/node_outputs/part-1/images-js/graph-non-linear-dataset.png" alt=""></p>
<p>The function would have to be something like that:</p>
<p><img src="/node_outputs/part-1/images-js/graph-non-linear-solution.png" alt=""></p>
<p>Not very linear... So one neuron can't solve this problem. Intuitively, it may
seems like using more neurons and layers may solve this problem. But because
neurons are linear functions, it won't help. Let's see why.</p>
<p>Let's begin with a deeper network:</p>
<p><img src="/node_outputs/part-1/images-js/neural-net-111-equation.png" alt=""></p>
<p>We can confirm that for different weights values it still output lines:</p>
<p><img src="/node_outputs/part-1/images-js/graph-nnet-111-weights.png" alt=""></p>
<p>Why ? If we take a closer look at the previous equation, we can see that both
weights are multiplied together and act like a single weight.</p>
<p><img src="/node_outputs/part-1/images-js/neural-net-111-equation-shrink.png" alt=""></p>
<p>No matter how deep our network is, it's equivalent to a network with a single
neuron.</p>
<p>What about a larger network ?</p>
<p><img src="/node_outputs/part-1/images-js/neural-net-121-equation.png" alt=""></p>
<p>Let's look at the generated functions:</p>
<p><img src="/node_outputs/part-1/images-js/graph-nnet-121-weights.png" alt=""></p>
<p>Once again, we are out of luck. The reason also lies in the equations:</p>
<p><img src="/node_outputs/part-1/images-js/neural-net-121-equation-shrink.png" alt=""></p>
<p><img src="/node_outputs/part-1/images-js/neural-net-121-equation-shrink-deep.png" alt=""></p>
<p><img src="/node_outputs/part-1/images-js/neural-net-121-equation-shrink-large.png" alt=""></p>
<p>No matter how large our network is, it's equivalent to a network with a single
neuron.</p>
<p>We just proved that our small networks composed of one neuron are equivalent to
bigger networks comprised of many layers and neurons. And, we also showed that
those networks can only solve linear problems.</p>
<p>This is because we combine linear functions: any combination of linear functions
produce a linear function. So we can add as many neurons as we want, it will
always output a line. (TODO)</p>
<p>We need to add some non-linearity in order to be able to solve problems like the
one above. It's time to introduce our saviours: activation functions.</p>
<p>TODO: 3d example with linear plane</p>
<script src="/node_sources/part-1/images-html/activ/activ.js"></script>
<h1>Activation functions</h1>
<p>Activation functions is a name given to functions applied to the output of
neurons.</p>
<p>Without activation, we sum the products of each input with its corresponding
weight.</p>
<p><img src="/node_outputs/part-1/images-js/nnet-21-equation.png" alt=""></p>
<p>Here is the corresponding computation graph:</p>
<p><img src="/node_outputs/part-1/images-js/nnet-21-computation.png" alt=""></p>
<p>With an activation, we also apply a function on the result:</p>
<p><img src="/node_outputs/part-1/images-js/nnet-21-equation-activ.png" alt=""></p>
<p><img src="/node_outputs/part-1/images-js/nnet-21-computation-activ.png" alt=""></p>
<h2>Code</h2>
<p>Enought words, let's code. In the first example we will create an activation
that do nothing, the result will be the same as without an activation.</p>
<pre><code class="language-python">w1 = <span class="hljs-number">0.4</span>
w2 = <span class="hljs-number">0.4</span>

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">neuron</span><span class="hljs-params">(x1, x2)</span>:</span>
    z  = x1 * w1
    z += x2 * w2
    <span class="hljs-keyword">return</span> z

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">do_nothing</span><span class="hljs-params">(z)</span>:</span>
    <span class="hljs-keyword">return</span> z

do_nothing(neuron(BLACK, WHITE))

<span class="hljs-comment">#&gt; 0.4</span>

do_nothing(neuron(WHITE, WHITE))

<span class="hljs-comment">#&gt; 0.8</span>
</code></pre>
<p>In the second example we will use an activation that square the output.</p>
<pre><code class="language-python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">square</span><span class="hljs-params">(z)</span>:</span>
    <span class="hljs-keyword">return</span> z * z

square(neuron(BLACK, WHITE))

<span class="hljs-comment">#&gt; 0.16</span>

square(neuron(WHITE, WHITE))

<span class="hljs-comment">#&gt; 0.64</span>
</code></pre>
<p>With keras there is two way to use activations. The first one is by using an
activation layer:</p>
<pre><code class="language-python"><span class="hljs-keyword">from</span> keras.layers <span class="hljs-keyword">import</span> Activation
</code></pre>
<p>It takes a single argument: the function to apply.</p>
<pre><code class="language-python">model = Sequential()
model.add(Dense(<span class="hljs-number">1</span>, use_bias = <span class="hljs-keyword">False</span>, input_shape = (<span class="hljs-number">2</span>,))
model.add(Activation(square))
</code></pre>
<p>The second, shorter version, is to use the <code>activation</code> parameter of a layer.</p>
<pre><code class="language-python">model = Sequential()
model.add(Dense(<span class="hljs-number">1</span>, use_bias = <span class="hljs-keyword">False</span>, input_shape = (<span class="hljs-number">2</span>,), activation = square))
</code></pre>
<p>If we reuse the weights of our previous example, we get the same predictions</p>
<pre><code class="language-python">model.set_weights([[[<span class="hljs-number">0.4</span>], [<span class="hljs-number">0.4</span>]]])

model.predict(np.array([[BLACK, WHITE]]))

<span class="hljs-comment">#&gt; array([[ 0.16 ]], dtype=float32)</span>

model.predict(np.array([[WHITE, WHITE]]))

<span class="hljs-comment">#&gt; array([[ 0.64 ]], dtype=float32)</span>
</code></pre>
<p>In both case the activation function is applied on all the neurons of the
previous layer.</p>
<pre><code class="language-python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">ActivationLayer</span><span class="hljs-params">(object)</span>:</span>

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, activation)</span>:</span>
        self.activation = activation

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">predict</span><span class="hljs-params">(self, inputs)</span>:</span>
        <span class="hljs-string">"""
        For each inputs
            apply the activation
        return all outputs
        """</span>
        outputs = []
        <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> inputs:
            outputs.append(self.activation(x))
        <span class="hljs-keyword">return</span> outputs
</code></pre>
<p>We now understand what are activation functions, but how do they help us ?</p>
<h1>Activation with hidden layers</h1>
<p>Remember our problem ? Due to our neurons producing linear outputs, adding more
neurons had no effect. Let's look at the output of a single neuron with a single
input.</p>
<p><img src="/node_outputs/part-1/images-js/nnet-11-linear.png" alt=""></p>
<script src="/node_sources/part-1/images-html/activ/nnet-11-linear.js"></script>
<p>If we use a non-linear activation, the output is no longer linear:</p>
<p><img src="/node_outputs/part-1/images-js/nnet-11-hidden-activ.png" alt=""></p>
<script src="/node_sources/part-1/images-html/activ/nnet-11.js"></script>
<p>Now if we stack one more neuron, we can learn a more complex, non linear function:</p>
<p><img src="/node_outputs/part-1/images-js/nnet-111-hidden-activ.png" alt=""></p>
<script src="/node_sources/part-1/images-html/activ/nnet-111.js"></script>
<p>And like that we can build big networks. This may seems like a small change but
this trick was the missing piece to learn arbitrary complex functions. Given
enought neurons, it has been proven that a network with a single hidden layer
and non-linear activation can represent any function. (search 'universal
approximation theorem' for more information)</p>
<p>But not all functions can be used as activation, there is some restrictions.
Fortunately we don't need to know them: In machine learning we juggle with a few
well studied activations, we just need to learn when to use them.</p>
<p>TODO: can we still learn linear function ?</p>
<h1>ReLu</h1>
<p>Nowadays, for hidden layers, a strong default is the Rectified Linear Unit or
ReLu for short.</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>m</mi><mi>i</mi><mi>n</mi><mo>(</mo><mn>0</mn><mo separator="true">,</mo><mi>x</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">min(0, x)
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base displaystyle textstyle uncramped"><span class="mord mathit">m</span><span class="mord mathit">i</span><span class="mord mathit">n</span><span class="mopen">(</span><span class="mord mathrm">0</span><span class="mpunct">,</span><span class="mord mathit">x</span><span class="mclose">)</span></span></span></span></span></p>
<script src="/node_sources/part-1/images-html/activ/relu.js"></script>
<p>With keras, we can also pass a string instead of a function for preregistered
activation.</p>
<pre><code class="language-python">model.add(Activation(<span class="hljs-string">"relu"</span>))
<span class="hljs-comment"># or</span>
model.add(Dense(..., activation = <span class="hljs-string">"relu"</span>))
</code></pre>
<h1>Activation with output layers</h1>
<p>Apart from being used on hidden layers to be able to build complex networks, activations
can also be used on output layers to format output values.</p>
<p>For example, for classification tasks we almost always want the output to be between 0
and 1: we don't care about other values. But currently, without activation, a
neuron can return any value.</p>
<p>There is an activation called sigmoid, that can help us solve this problem, so
let's take a look at it.</p>
<h1>Sigmoid</h1>
<p>(margin &amp; issue with exponent)</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mfrac><mrow><mn>1</mn></mrow><mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi><mrow><mo>−</mo><mi>x</mi></mrow></msup></mrow></mfrac></mrow><annotation encoding="application/x-tex">\dfrac{1}{1 + e^{-x}}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:1.32144em;"></span><span class="strut bottom" style="height:2.09077em;vertical-align:-0.7693300000000001em;"></span><span class="base displaystyle textstyle uncramped"><span class="mord reset-textstyle displaystyle textstyle uncramped"><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span><span class="mfrac"><span class="vlist"><span style="top:0.686em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle textstyle cramped"><span class="mord textstyle cramped"><span class="mord mathrm">1</span><span class="mbin">+</span><span class="mord"><span class="mord mathit">e</span><span class="vlist"><span style="top:-0.289em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord">−</span><span class="mord mathit">x</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span><span style="top:-0.22999999999999998em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle textstyle uncramped frac-line"></span></span><span style="top:-0.677em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle textstyle uncramped"><span class="mord textstyle uncramped"><span class="mord mathrm">1</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span></span></span></span></span></span></p>
<script src="/node_sources/part-1/images-html/activ/sigmoid.js"></script>
<p>As we can see, its output is constrained between 0 and 1, so we will never get
undesirable values.</p>
<p>The sigmoid function was the de-facto activation used everywhere a few years
ago. Nowadays we mostly use it when we need an output between 0 and 1. (~~~)</p>
<p>For now, this activation will be enought, so let's move on.</p>
<h1>Code</h1>
<p>We now have learned enought to be able to build powerful networks. So let's put
this knowledge into practice by building a useful model that detect edges: given
two pixels:</p>
<ul>
<li>return 1 if both are of different colors</li>
<li>return 0 otherwise</li>
</ul>
<p>First let's confirm we couldn't solve this problem without activations.</p>
<p>TODO: visualize function as non linear 1d concat or 3d</p>
<p>Now, let's think about which activations we want use. As said before, for hidden
layers, ReLu is currently the preferred choice. As for the output we want to
return 1 if there is a border and 0 otherwise. So we can use a sigmoid
activation to avoid output values like 5 or -2.</p>
<p>There is no easy way to choose the number of neurons and layers, we have to try
some configurations. Here this is a simple problem, so we don't need a big
network. I tried 1 layer with 8 neurons and it worked, so let's use that.</p>
<p>For the output, we only have one category (border or not), so one output neuron
is enought.</p>
<p>TODO: picture ?</p>
<p>Here is the code, you should be familiar with:</p>
<pre><code class="language-python">border_m = Sequential()
border_m.add(Dense(<span class="hljs-number">8</span>, activation = <span class="hljs-string">"relu"</span>, input_shape = (<span class="hljs-number">2</span>,)))
border_m.add(Dense(<span class="hljs-number">1</span>, activation = <span class="hljs-string">"sigmoid"</span>))

border_x = [[BLACK, BLACK], [WHITE, BLACK], [WHITE, WHITE], [BLACK, WHITE]]
border_y = [<span class="hljs-number">0</span>,              <span class="hljs-number">1</span>,              <span class="hljs-number">0</span>,              <span class="hljs-number">1</span>]

train(
    border_m,
    border_x,
    border_y
)

<span class="hljs-comment">#&gt; ...</span>
<span class="hljs-comment">#&gt; Epoch 1999/2000</span>
<span class="hljs-comment">#&gt; 4/4 [==============================] - 0s - loss: 0.1216 - acc: 1.0000</span>
<span class="hljs-comment">#&gt; Epoch 2000/2000</span>
<span class="hljs-comment">#&gt; 4/4 [==============================] - 0s - loss: 0.1215 - acc: 1.0000</span>

border_m.predict_classes(border_x)

<span class="hljs-comment">#&gt; array([[0],</span>
<span class="hljs-comment">#&gt;        [1],</span>
<span class="hljs-comment">#&gt;        [0],</span>
<span class="hljs-comment">#&gt;        [1]], dtype=int32)</span>

border_m.predict(border_x)

<span class="hljs-comment">#&gt; array([[ 0.42065826],</span>
<span class="hljs-comment">#&gt;        [ 0.63238412],</span>
<span class="hljs-comment">#&gt;        [ 0.33127135],</span>
<span class="hljs-comment">#&gt;        [ 0.74696058]], dtype=float32)</span>
</code></pre>
<p>And it works ! As usual, let's code what happened, we will see that there is no
black magic involved.</p>
<p>TODO:</p>
<pre><code class="language-python">w1 = border_m.layers[<span class="hljs-number">0</span>].get_weights()[<span class="hljs-number">0</span>]
b1 = border_m.layers[<span class="hljs-number">0</span>].get_weights()[<span class="hljs-number">1</span>]
w2 = border_m.layers[<span class="hljs-number">1</span>].get_weights()[<span class="hljs-number">0</span>]
b2 = border_m.layers[<span class="hljs-number">1</span>].get_weights()[<span class="hljs-number">1</span>]

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">dense</span><span class="hljs-params">(n, x, w, b, activation)</span>:</span>
    z = np.dot(x, w) + b
    a = activation(z)
    <span class="hljs-keyword">return</span> z

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">relu</span><span class="hljs-params">(x)</span>:</span>
    <span class="hljs-keyword">return</span> np.maximum(x, <span class="hljs-number">0</span>)

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">sigmoid</span><span class="hljs-params">(x)</span>:</span>
    <span class="hljs-keyword">return</span> <span class="hljs-number">1</span> / (<span class="hljs-number">1</span> + np.exp(-x))

x = [BLACK, WHITE]
x = dense(<span class="hljs-number">8</span>, x, w1, b1, relu)
y = dense(<span class="hljs-number">1</span>, x, w2, b2, sigmoid)
y

<span class="hljs-comment">#&gt; array([ 0.74696058])</span>
</code></pre>
<h1>Designing bigger networks</h1>
<p>Let's end this section with some words about designing neural nets.</p>
<p>The first thing to know is that nobody knows the best number of neurons and
layers for your network. You always have to try different architectures and
keep the one that work best for your problem.</p>
<p>Now, why use layers at all ? As said before, it has been proven that a network
with a single hidden layer can approximate any function, so why not just add
more neurons and forget about layers ? Well, while it can learn anything, it
doesn't mean it's efficient.  We've seen that neurons can compute simple
functions, so to perform a complex tasks like recognizing an animal in a
picture, you need lots and lots of neurons.
But if instead you use multiple layers, you can make the problem much more
easier:</p>
<ul>
<li>the 1st layer detect the edges and the colors</li>
<li>the 2nd layer use the edges to detect more complex shapes like lines,
squares, circles...</li>
<li>the 3rd layer can now learn to recognize eyes, legs, arms...</li>
<li>etc...</li>
</ul>
<p>Using more layers will require far less neurons and learn more quickly and
accurately. Note that you still need to have enought neurons on a layer so that
subsequent layers have enought data to work on.</p>
<p>So now, the question you may ask is why don't we go deeper and deeper ? Well
training deep networks (deep learning) is not easy. Until a few years ago it was
very hard to train networks with more than a few layers and we didn't have the
computation power to do it. But thanks to the rise in computing capabilities and
research in machine learning, we can now train bigger networks that can have
hundreds of layers. As we will see deep learning, is basically just tricks to
make neural networks work better and faster.</p>
<script src="/node_sources/part-1/images-html/train/invsign.js"></script>
<h1>Gradient Descent: the magic</h1>
<p>Gradient descent is an algorithm used to train networks. It's a key component
of machine learning and we are about to learn how it works !</p>
<p>In order to keep the math simple, we will use a new problem: given a number,
output that same number with its sign reversed. We will also use a small dataset
comprised of only two values:</p>
<ul>
<li><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>x</mi><mo>=</mo><mo>+</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">x = +1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.64444em;"></span><span class="strut bottom" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="base textstyle uncramped"><span class="mord mathit">x</span><span class="mrel">=</span><span class="mord">+</span><span class="mord mathrm">1</span></span></span></span>, <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>y</mi><mo>=</mo><mo>−</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">y = -1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.64444em;"></span><span class="strut bottom" style="height:0.8388800000000001em;vertical-align:-0.19444em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.03588em;">y</span><span class="mrel">=</span><span class="mord">−</span><span class="mord mathrm">1</span></span></span></span></li>
<li><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>x</mi><mo>=</mo><mo>−</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">x = -1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.64444em;"></span><span class="strut bottom" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="base textstyle uncramped"><span class="mord mathit">x</span><span class="mrel">=</span><span class="mord">−</span><span class="mord mathrm">1</span></span></span></span>, <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>y</mi><mo>=</mo><mo>+</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">y = +1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.64444em;"></span><span class="strut bottom" style="height:0.8388800000000001em;vertical-align:-0.19444em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.03588em;">y</span><span class="mrel">=</span><span class="mord">+</span><span class="mord mathrm">1</span></span></span></span></li>
</ul>
<div class="note">
<p>This is not a classification problem but a regression problem.</p>
</div>
<p>For now, let's forget about activations and biases: a single neuron is enought to
solve this problem.</p>
<p><img src="/node_outputs/part-1/images-js/nnet-sign.png" alt=""></p>
<p>And here is what we expect:</p>
<p><img src="/node_outputs/part-1/images-js/nnet-sign-dataset.png" alt=""></p>
<p>As said before, training a network is just finding finding the good weights. So
let's do that !</p>
<p>First we need to initialize our network: we need to give the weight an initial
value. There is many different methods to do so, but we have no interest in them
in this post. So let's use an arbitrary value:</p>
<p><img src="/node_outputs/part-1/images-js/nnet-sign-init.png" alt=""></p>
<p>Once initialized, we can use our network to make some predictions:</p>
<p><img src="/node_outputs/part-1/images-js/nnet-sign-init-dataset.png" alt=""></p>
<p>And as expected, this is not what we wanted. (todo: center, labels, title &amp;
tooltips...)</p>
<script>renderInvsign(+2, undefined, false)</script>
<p>So, how bad are our predictions ? We need a way to know how wrong we are. One
way to do that is to compute the distances between our predictions and the
expected outputs: subtract the labels to our predictions.
(rephrase...)</p>
<p><img src="/node_outputs/part-1/images-js/nnet-sign-init-error-sub.png" alt=""></p>
<p>The distances we compute correspond to the red lines on the figure below:</p>
<script>renderInvsign(+2)</script>
<p>We can see that the distances can be either positives or negatives. But we don't
care about the sign: we are only interested by how much. So we square the result
to get positive distances only.</p>
<p><img src="/node_outputs/part-1/images-js/nnet-sign-init-error.png" alt=""></p>
<p>Finally, let's sum all the errors so that we can have a single value
representing the total error of the network: <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>9</mn><mo>+</mo><mn>9</mn><mo>=</mo><mn>1</mn><mn>8</mn></mrow><annotation encoding="application/x-tex">9 + 9 = 18</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.64444em;"></span><span class="strut bottom" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="base textstyle uncramped"><span class="mord mathrm">9</span><span class="mbin">+</span><span class="mord mathrm">9</span><span class="mrel">=</span><span class="mord mathrm">1</span><span class="mord mathrm">8</span></span></span></span>.</p>
<p>Let's plot it (add distance with weight &amp; color current weight)</p>
<script>renderInvsignModel(+2)</script>
<p>Ok, now we know how to compute the error of our network, what are we going to
use it for ?</p>
<p>Let's take some time to experiment: what would happen if we used a bigger weight
?</p>
<p><img src="/node_outputs/part-1/images-js/nnet-sign-init-error-inc.png" alt=""></p>
<p>The error would go up: <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>1</mn><mn>6</mn><mo>+</mo><mn>1</mn><mn>6</mn><mo>=</mo><mn>3</mn><mn>2</mn></mrow><annotation encoding="application/x-tex">16 + 16 = 32</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.64444em;"></span><span class="strut bottom" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="base textstyle uncramped"><span class="mord mathrm">1</span><span class="mord mathrm">6</span><span class="mbin">+</span><span class="mord mathrm">1</span><span class="mord mathrm">6</span><span class="mrel">=</span><span class="mord mathrm">3</span><span class="mord mathrm">2</span></span></span></span>. We can also see that the distance between
the predictions and the labels increased: our network makes worse predictions.</p>
<script>renderInvsignModel(+3)</script>
<p>What if we used a smaller weight ?</p>
<p><img src="/node_outputs/part-1/images-js/nnet-sign-init-error-dec.png" alt=""></p>
<p>The error would go down: <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>4</mn><mo>+</mo><mn>4</mn><mo>=</mo><mn>8</mn></mrow><annotation encoding="application/x-tex">4 + 4 = 8</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.64444em;"></span><span class="strut bottom" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="base textstyle uncramped"><span class="mord mathrm">4</span><span class="mbin">+</span><span class="mord mathrm">4</span><span class="mrel">=</span><span class="mord mathrm">8</span></span></span></span>. The generated function pass closer to our
dataset points: our network makes better predictions.</p>
<script>renderInvsignModel(+1)</script>
<p>Reducing the weight improved our predictions, let's do that again !</p>
<script>renderInvsignModel(+0)</script>
<p>And once again our error decreased ! Let's not stop here !</p>
<script>renderInvsignModel(-1)</script>
<p>Our error is now 0: we just found the perfect weight for this problem: the
function fit perfectly our dataset.</p>
<p>What happens if we keep decreasing the weight ? Well, from this point, the error
will get bigger and bigger.</p>
<script src="/node_sources/part-1/images-html/train/invsign-error-animated.js"></script>
<div class="important">
<p>the lower the error, the better our network fit our dataset.</p>
</div>
<p>Now that we understand how to compute errors and what they represent, let's draw
all of them:</p>
<script src="/node_sources/part-1/images-html/train/invsign-error.js"></script>
<p>We can see that the curve has a bowl like shape. Let's imagine that this curve
represent a hill. You're gone for a hike and you're lost: you are currently at a
random position on the hill (the initial weight value).</p>
<script src="/node_sources/part-1/images-html/train/invsign-descent.js"></script>
<script>renderInvsignDescent(+2.0)</script>
<p>You want to go back to your home (the perfect weight) but unfortunately there is
fog: you can't see your house nor know where you are on the hill. Fortunately,
you know your house is at the bottom of the hill and you can see enought to know
the direction of the slope. So you look around and take step towards the bottom
(you improve your weight). (TODO: hide part of the curve and explain lr ?)</p>
<script>renderInvsignDescent(+1.5)</script>
<p>And you repeat this, again and again, until you reach your house (the perfect
weight value) (todo: slope in red)</p>
<script src="/node_sources/part-1/images-html/train/invsign-descent-animated.js"></script>
<p>No matter where you start, by following the direction of the slope, you will
eventually reach the bottom.</p>
<p>And that's exactly how our algorithm works ! This is why it's called gradient
descent (gradient is another word for slope).</p>
<p>Now we got the big picture, let's implement it !</p>
<h2>Math</h2>
<p>The algorithm in itself is just a big loop. The tricky part is getting the
direction of the slope at our current position.</p>
<p>Fortunately for us there is a mathematical tool that just do that: the
derivatives ! Let's take some time to understand how they work.</p>
<div class="note">
<p>Computing the exact values of the derivatives would just make things more
complicated. Instead we will use an approximation that's easier to understand
and gives us almost the same results.</p>
</div>
<p>As said before a derivative compute a slope. So let's review how we compute a
slope between two points <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>a</mi></mrow><annotation encoding="application/x-tex">a</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">a</span></span></span></span> and <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>b</mi></mrow><annotation encoding="application/x-tex">b</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.69444em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">b</span></span></span></span>. As a convention <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>b</mi></mrow><annotation encoding="application/x-tex">b</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.69444em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">b</span></span></span></span> is on the right of <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>a</mi></mrow><annotation encoding="application/x-tex">a</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">a</span></span></span></span>.</p>
<ul>
<li>measure the altitude between both points: <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>y</mi><mi>b</mi><mo>−</mo><mi>y</mi><mi>a</mi></mrow><annotation encoding="application/x-tex">yb - ya</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.03588em;">y</span><span class="mord mathit">b</span><span class="mbin">−</span><span class="mord mathit" style="margin-right:0.03588em;">y</span><span class="mord mathit">a</span></span></span></span></li>
<li>then divide by the distance between the two points: <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>x</mi><mi>b</mi><mo>−</mo><mi>x</mi><mi>a</mi></mrow><annotation encoding="application/x-tex">xb - xa</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.77777em;vertical-align:-0.08333em;"></span><span class="base textstyle uncramped"><span class="mord mathit">x</span><span class="mord mathit">b</span><span class="mbin">−</span><span class="mord mathit">x</span><span class="mord mathit">a</span></span></span></span></li>
</ul>
<p>(add lines...)</p>
<p><img src="/node_outputs/part-1/images-js/gd-graph-slope-neg.png" alt=""></p>
<p>If the result is negative, it means that if you go from <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>a</mi></mrow><annotation encoding="application/x-tex">a</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">a</span></span></span></span> to <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>b</mi></mrow><annotation encoding="application/x-tex">b</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.69444em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">b</span></span></span></span>, you go down.</p>
<p><img src="/node_outputs/part-1/images-js/gd-graph-slope-pos.png" alt=""></p>
<p>If the result is positive, it means that if you go from <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>a</mi></mrow><annotation encoding="application/x-tex">a</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">a</span></span></span></span> to <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>b</mi></mrow><annotation encoding="application/x-tex">b</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.69444em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">b</span></span></span></span>, you go up.</p>
<p><img src="/node_outputs/part-1/images-js/gd-graph-slope-pos-steep.png" alt=""></p>
<p>The bigger the value, the steeper the slope</p>
<p>Now there is a little problem, our derivatives compute the slope at a given
point: there is no second point ! In fact a derivative compute the slope between
the given point and a second very very close point. So given a point <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">x</span></span></span></span>, we can
approximate the derivative as:</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>d</mi><mi>e</mi><mi>r</mi><mi>i</mi><mi>v</mi><mi>a</mi><mi>t</mi><mi>i</mi><mi>v</mi><mi>e</mi><mo>(</mo><mi>x</mi><mo>)</mo><mo>=</mo><mi>s</mi><mi>l</mi><mi>o</mi><mi>p</mi><mi>e</mi><mo>(</mo><mi>x</mi><mo separator="true">,</mo><mi>x</mi><mo>+</mo><mn>0</mn><mi mathvariant="normal">.</mi><mn>0</mn><mn>0</mn><mn>0</mn><mn>1</mn><mo>)</mo></mrow><annotation encoding="application/x-tex">derivative(x) = slope(x, x + 0.0001)
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base displaystyle textstyle uncramped"><span class="mord mathit">d</span><span class="mord mathit">e</span><span class="mord mathit" style="margin-right:0.02778em;">r</span><span class="mord mathit">i</span><span class="mord mathit" style="margin-right:0.03588em;">v</span><span class="mord mathit">a</span><span class="mord mathit">t</span><span class="mord mathit">i</span><span class="mord mathit" style="margin-right:0.03588em;">v</span><span class="mord mathit">e</span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mclose">)</span><span class="mrel">=</span><span class="mord mathit">s</span><span class="mord mathit" style="margin-right:0.01968em;">l</span><span class="mord mathit">o</span><span class="mord mathit">p</span><span class="mord mathit">e</span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mpunct">,</span><span class="mord mathit">x</span><span class="mbin">+</span><span class="mord mathrm">0</span><span class="mord mathrm">.</span><span class="mord mathrm">0</span><span class="mord mathrm">0</span><span class="mord mathrm">0</span><span class="mord mathrm">1</span><span class="mclose">)</span></span></span></span></span></p>
<p>Another way to think about derivatives is: given a point, how will the output
change if i move a tiny bit to the right ?</p>
<h1>code</h1>
<p>It's time to implement what we've seen ! Let's begin with our usual neuron
function.</p>
<pre><code class="language-python"><span class="hljs-comment"># initial weight</span>
weight = <span class="hljs-number">2</span>

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">neuron</span><span class="hljs-params">(x, w)</span>:</span>
    <span class="hljs-keyword">return</span> x * w

neuron(<span class="hljs-number">-1</span>, weight)

<span class="hljs-comment">#&gt; -2</span>

neuron(+<span class="hljs-number">1</span>, weight)

<span class="hljs-comment">#&gt; +2</span>
</code></pre>
<p>Next we compute the error for a single prediction.</p>
<pre><code class="language-python"><span class="hljs-comment"># our dataset</span>
inputs = [<span class="hljs-number">-1</span>, +<span class="hljs-number">1</span>]
labels = [+<span class="hljs-number">1</span>, <span class="hljs-number">-1</span>]

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">neuron_error</span><span class="hljs-params">(prediction, label)</span>:</span>
    <span class="hljs-string">"""
    distance between our prediction and the label
    """</span>
    <span class="hljs-keyword">return</span> (prediction - label) ** <span class="hljs-number">2</span>

p = neuron(inputs[<span class="hljs-number">0</span>], weight)
neuron_error(p, labels[<span class="hljs-number">0</span>])

<span class="hljs-comment">#&gt; 9</span>

p = neuron(inputs[<span class="hljs-number">1</span>], weight)
neuron_error(p, labels[<span class="hljs-number">1</span>])

<span class="hljs-comment">#&gt; 9</span>
</code></pre>
<p>Then we compute the total error of a network given some weights.</p>
<pre><code class="language-python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">network_error</span><span class="hljs-params">(weight)</span>:</span>
    <span class="hljs-string">"""
    for each input in our dataset
        predict and compute the error
    return the sum of the errors
    """</span>
    error = <span class="hljs-number">0</span>
    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(len(inputs)):
        p      = neuron(inputs[i], weight)
        error += neuron_error(p, labels[i])
    <span class="hljs-keyword">return</span> error

network_error(weight)

<span class="hljs-comment">#&gt; 18</span>
</code></pre>
<p>The next function compute the slope between two network errors. The two points
<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">x</span></span></span></span> are two weights and their value <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.03588em;">y</span></span></span></span> is their corresponding network error.</p>
<pre><code class="language-python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">slope</span><span class="hljs-params">(wa, wb)</span>:</span>
    xa = wa
    xb = wb
    ya = network_error(xa)
    yb = network_error(xb)
    <span class="hljs-keyword">return</span> (yb - ya) / (xb - xa)
</code></pre>
<p>And now we implement the derivative that give us the slope for a given weight
value. Remember that a derivative is almost equivalent to computing the slope
between the given point and a very close point.</p>
<pre><code class="language-python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">derivative</span><span class="hljs-params">(w)</span>:</span>
    <span class="hljs-keyword">return</span> slope(w, w + <span class="hljs-number">0.0001</span>)
</code></pre>
<p>Let's check that our function works by getting the slope at our initial weight
value:</p>
<pre><code class="language-python">derivative(weight)

<span class="hljs-comment">#&gt; 12.000199999998785</span>
</code></pre>
<p>We can see it's positive, i.e that if we increase the weight, the error increase
too. Exactly what we could observe with the error curve.</p>
<p>Now, let's fit all the pieces together ! If the derivative is positive, that
means if we go right (increase the weight) the error increase. In this case we
would like to decrease the weight to reduce the error:</p>
<p><img src="/node_outputs/part-1/images-js/gd-graph-slope-pos.png" alt=""></p>
<p>If the derivative is negative, that means if we go right (increase the weight)
the error decrease.</p>
<p><img src="/node_outputs/part-1/images-js/gd-graph-slope-neg.png" alt=""></p>
<p>Let's summarize:</p>
<ul>
<li>derivative is positive -&gt; decrease the weight</li>
<li>derivative is negative -&gt; increase the weight</li>
</ul>
<p>One simple way to do that is to subtract the value of the derivative from the
weight.</p>
<ul>
<li>derivative = +1 -&gt; w - (+1) = w - 1 -&gt; decrease the weight</li>
<li>derivative = -1 -&gt; w - (-1) = w + 1 -&gt; increase the weight</li>
</ul>
<p>Implementing the step function is pretty straightforward:</p>
<pre><code class="language-python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">step</span><span class="hljs-params">(w)</span>:</span>
    s  = derivative(w)
    w -= s
    <span class="hljs-keyword">return</span> w
</code></pre>
<p>Now let's take a step, we have currently <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>w</mi><mo>=</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">w = 2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.64444em;"></span><span class="strut bottom" style="height:0.64444em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.02691em;">w</span><span class="mrel">=</span><span class="mord mathrm">2</span></span></span></span> and we know that the optimal
weight is <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>w</mi><mo>=</mo><mo>−</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">w = -1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.64444em;"></span><span class="strut bottom" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.02691em;">w</span><span class="mrel">=</span><span class="mord">−</span><span class="mord mathrm">1</span></span></span></span>. So a step should decrease the weight to make it closer to
-1.</p>
<pre><code class="language-python">weight

<span class="hljs-comment">#&gt; 2</span>

weight = step(weight)
weight

<span class="hljs-comment">#&gt; -10</span>
</code></pre>
<p>Wow, what happened ? We wanted to take small steps until we reached -1, the
perfect weight, but instead we took a giant step, and we overshoot the bottom.</p>
<script src="/node_sources/part-1/images-html/train/invsign-overshoot.js"></script>
<p>TODO: explain why ? big/small steps and why steps</p>
<p>To prevent that we use what's called a learning rate. It's a number we multiply
with the derivative value that allows us to control the step size. In this case
we would like to take smaller steps, so we use a small number:</p>
<pre><code class="language-python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">step</span><span class="hljs-params">(w, learning_rate = <span class="hljs-number">0.01</span>)</span>:</span>
    s  = derivative(w)
    w -= s * learning_rate
    <span class="hljs-keyword">return</span> w
</code></pre>
<p>Now let's start again</p>
<pre><code class="language-python">weight = <span class="hljs-number">2</span>
weight = step(weight)
weight

<span class="hljs-comment">#&gt; 1.8</span>
</code></pre>
<p>That's better, so let's take 100 more steps:</p>
<pre><code class="language-python"><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">100</span>):
    weight = step(weight)

weight

<span class="hljs-comment">#&gt; -0.95</span>
</code></pre>
<p>Nice almost the good weight -1. Now you understand why our network never find
the exact perfect weight. We have to take small steps, and we can only become
closer and closer to it without ever reaching it.</p>
<p>Now let's see our network prediction now:</p>
<pre><code class="language-python">neuron(<span class="hljs-number">-1</span>, weight)

<span class="hljs-comment">#&gt; 0.95</span>

neuron(+<span class="hljs-number">1</span>, weight)

<span class="hljs-comment">#&gt; -0.95</span>
</code></pre>
<p>And this is it ! We could run a few more steps to have a better accuracy, but
its not very interesting. (TODO: stop when you have satisfying results)</p>
<p>You will often hear that neural networks are functions approximator. It just
refer to what we have seen: neural networks represent a function, and each time
we improve the weights the function fit better the dataset but never exactly. So
they approximate the desired function.</p>
<h2>Gradient descent: more</h2>
<p>Ok, now we know of how to train a small network with only one weight, but how
does this work with more complex networks that have multiple inputs, neurons,
biases and layers ? The only thing that changes, is that you now have more
weights. For each one, you repeat the same procedure we did: how does the error
change if the weight is increased a bit ? Then update the weight to decrease the
error.</p>
<p>To compute the slope for a specific weight, freeze all other weights. Here is an
example for computing the derivative of the second weight of a network with 4
weights:</p>
<pre><code class="language-python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">derivative_w2</span><span class="hljs-params">(w1, w2, w3, w4)</span>:</span>
    xa = w2
    xb = w2 + <span class="hljs-number">0.0001</span>

    yb = network_error(w1, xb, w3, w4)
    ya = network_error(w1, xa, w3, w4)

    <span class="hljs-keyword">return</span> (yb - ya) / (xb - xa)
</code></pre>
<p>Compute the update for all the weights then update them all at once.</p>
<p>talk about 2d/3d</p>
<p>Computing the derivative for each weights is heavy. Instead we use a method
called backpropagation, that reuse previous derivative to speed things up. It
begins by computing the derivatives of the last layer and then use those to
compute the derivatives of the previous layer etc.... This is called the
backward pass because it goes from right to left in contrast with the forward
pass. (forward = prediction, backward = training)</p>
<p><img src="/node_outputs/part-1/images-js/nnet-2332-backward.png" alt=""></p>
<p>Now what if we have activations ? well, activations have no weights, so there is
nothing to compute. But remember when we said activations had some constraints ?
Well one of them is that the 'hill' shape must be preserved. A bad activation
will output a curve that won't work with our algorithm. Here is such a case:</p>
<script src="/node_sources/part-1/images-html/train/step.js"></script>
<p>The slope is 0 almost everywhere: you will never know in which direction you
must update your weight.</p>
<p>But as you'll be using well known activations, you will never have this problem.</p>
<p>You just got an intuition behind one of the most important algorithm in machine
learning ! It's so important that I'll cover it in more details in a dedicated
post later in this series.</p>
<p>Now, let's take a last detour to wrap things up before doing the final project.
The hardest part is behind you, almost there =)</p>
<p>TODO: little summary</p>
<h1>Training</h1>
<p>With what we learned we can explain most of the <code>train</code> function. First let's
take a look at:</p>
<h2>compile</h2>
<pre><code class="language-python">model.compile(optimizer = <span class="hljs-string">"sgd"</span>, loss = <span class="hljs-string">"mse"</span>, metrics = [<span class="hljs-string">"accuracy"</span>])
</code></pre>
<p>This line configure our model for training</p>
<h2>loss</h2>
<p>As we saw earlier, we need a function to evaluate how well our network perform.
Those functions are called loss or cost functions.</p>
<p>Here we used MSE, which is the short for Mean Squared Error. Its the one we
described and used in our example.</p>
<p>Like activations, there is a few well known losses that we will cover in the
next section. Note that unlike activations it's not rare to create your own loss.</p>
<h2>optimizer</h2>
<p>Optimizers are the functions used to update the weights. In our example we
implemented Stochastic Gradient Descent or SGD for short.  There is several
other optimizers that all use the derivatives but update the weight in a
different way to improve speed and accuracy. Nowadays a good default is adam.</p>
<h2>metrics</h2>
<p>Metrics is just an array that specify what information should be displayed while
training. &quot;accuracy&quot; represent the percentage of correctly classified input</p>
<ul>
<li>1.0000: 100% of the inputs have been correctly classified</li>
<li>0.7500:  75% of the inputs have been correctly classified</li>
</ul>
<pre><code class="language-python"><span class="hljs-comment">#&gt; 2/2 [==============================] - 0s - loss: 1.0960e-07 - acc: 1.0000</span>
</code></pre>
<p>Here we can see that 'acc' is displayed. By default only the loss is displayed.</p>
<p>Until now the dataset represented all the value we wanted to predict, but in
real applications, we only have some examples. the objective is to have enought
different examples so that the function we learn will predict good values for
unseen inputs.</p>
<p>TODO: graph</p>
<h2>fit</h2>
<pre><code class="language-python">model.fit(inputs, labels, epochs = <span class="hljs-number">2000</span>)
</code></pre>
<p>This line actually train our model.</p>
<p>We are already familiar with inputs and labels, those are the pairs used to
train our network.</p>
<p>Epochs is the number of steps we want to take, i.e the number of times we
improve our weights.</p>
<p>You should now be able to understand the whole output:</p>
<pre><code class="language-python"><span class="hljs-comment">#&gt; Epoch 2000/2000</span>
<span class="hljs-comment">#&gt; 2/2 [==============================] - 0s - loss: 1.0960e-07 - acc: 1.0000</span>
</code></pre>
<p>You now understand our whole code !</p>
<h1>Output layers</h1>
<p>Until now we only predicted a single number. But neural networks can be used to predict
much more complex data. Let's see how we can do that !</p>
<h1>regression</h1>
<p>The ouput layer is used to give you the information you want. Here is a network
with a single output:</p>
<p><img src="/node_outputs/part-1/images-js/nnet-3331.png" alt=""></p>
<p>The most simple thing that an output neuron can return is a number. Let's see a
few example:</p>
<ul>
<li>given the size of a house, its number of rooms, predict its price.</li>
<li>given an image, predict the x position of an object</li>
<li>given an image, predict the y position of an object</li>
</ul>
<p>Instead of having two networks, one predicting the x coordinate and a second
predicting the y coordinate, you can use two neurons on the output layer and
create a single network that predict the x and y value at once:</p>
<p><img src="/node_outputs/part-1/images-js/nnet-3332.png" alt=""></p>
<p>The kind of tasks where you try to predict a continous value (like when we
inverted the sign of the input number), represent regression problems. For these
problems use one neuron per value you want to predict and use a mean squared
error loss.</p>
<p>When having multiple outputs with keras instead of having a label represented by
a single value, we use an array:</p>
<pre><code class="language-python"><span class="hljs-comment"># one output</span>
labels = [
    <span class="hljs-number">10</span>, <span class="hljs-comment"># label for input 1</span>
    <span class="hljs-number">20</span>, <span class="hljs-comment"># label for input 2</span>
    <span class="hljs-number">30</span>, <span class="hljs-comment"># label for input 3</span>
]

<span class="hljs-comment"># two outputs</span>
labels = [
    [<span class="hljs-number">10</span>, <span class="hljs-number">40</span>], <span class="hljs-comment"># label for input 1</span>
    [<span class="hljs-number">20</span>, <span class="hljs-number">50</span>], <span class="hljs-comment"># label for input 2</span>
    [<span class="hljs-number">30</span>, <span class="hljs-number">60</span>], <span class="hljs-comment"># label for input 3</span>
]
</code></pre>
<h1>classification</h1>
<h1>binary classification</h1>
<p>With one neuron we can also do binary classification: that's what we did until
now. Binary because we classify the input between two categories/classes:</p>
<ul>
<li>given a scan, predict if it's cancer or not cancer.</li>
<li>given an email, predict if it's a spam or not a spam.</li>
</ul>
<p>In these tasks, the return value is called the confidence. This is because you
often want to return 1 when you are 100% sure that the input belongs to the
current class and 0 when you are 100% sure that the input do not belongs to the
class. But you can also return intermediate value to express how much confident
you are in your response. In a spam system:</p>
<ul>
<li>confidence = 0.20 (20%): the mail is unlikely to be a spam (80% it is not)</li>
<li>confidence = 0.50 (50%): we don't know whether this is a spam or not</li>
<li>confidence = 0.90 (90%): the mail is almost certainly a spam</li>
</ul>
<p>To enforce a value between 0 and 1 (having a confidence of 110% is not very
helpful) we often use a sigmoid activation. As for the loss we can use
<code>binary_crossentropy</code>. We don't need to understand what is does, we just need to
know that it's a loss that works particularly well for this kind of problems.</p>
<p>Like for regression, if your input can belongs to multiple classes, you just have
to add more neurons on the output layer.</p>
<p>TODO: image</p>
<div class="note">
<p>In this tutorial we used <code>mean_squared_error</code> all along, but we should have been
using <code>binary_crossentropy</code> because we were doing binary classification.
Unfortunately it would have required bigger networks from the start and I wanted
to avoid that.</p>
</div>
<h1>multiclass classification</h1>
<p>We've just covered binary classification, but in fact it's just a special case
of classification. Instead of using one neuron we could have represented our
problem with two neurons:</p>
<ul>
<li>one that predicts the confidence of being a spam</li>
<li>one that predicts the confidence of not being a spam</li>
</ul>
<p>TODO: image</p>
<p>In that case, we should have used another loss: <code>categorical_crossentropy</code>.</p>
<p>But note that there is something else we need to take care of. With only one
neuron, if we had 80% confidence in the mail being a spam, we could easely
compute the confidence of the mail not being a spam with <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>1</mn><mi mathvariant="normal">.</mi><mn>0</mn><mn>0</mn><mo>−</mo><mn>0</mn><mi mathvariant="normal">.</mi><mn>8</mn><mn>0</mn><mo>=</mo><mn>0</mn><mi mathvariant="normal">.</mi><mn>2</mn><mn>0</mn></mrow><annotation encoding="application/x-tex">1.00 - 0.80 = 0.20</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.64444em;"></span><span class="strut bottom" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="base textstyle uncramped"><span class="mord mathrm">1</span><span class="mord mathrm">.</span><span class="mord mathrm">0</span><span class="mord mathrm">0</span><span class="mbin">−</span><span class="mord mathrm">0</span><span class="mord mathrm">.</span><span class="mord mathrm">8</span><span class="mord mathrm">0</span><span class="mrel">=</span><span class="mord mathrm">0</span><span class="mord mathrm">.</span><span class="mord mathrm">2</span><span class="mord mathrm">0</span></span></span></span>.</p>
<p>Now each neuron can output a value between 0 and 1. We could end up with both of
them outputing a 80% confidence. Being 80% sure the mail is a spam and 80% sure
the mail is not a spam is just meaningless. We need a way to ensure that both
probabilities sum to 1.</p>
<p>And once again there is a simple solution: using a softmax activation. Let's
take an example to see how it works. Each neuron can output any number: (TODO:
images)</p>
<p>neuron 1 = 30<br>
neuron 2 = 90</p>
<p>Those values are then passed to the softmax layer. A bigger value should have a
bigger confidence and a lower value should have a lower confidence. We begin by
summing all the values:</p>
<p><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>3</mn><mn>0</mn><mo>+</mo><mn>9</mn><mn>0</mn><mo>=</mo><mn>1</mn><mn>2</mn><mn>0</mn></mrow><annotation encoding="application/x-tex">30 + 90 = 120</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.64444em;"></span><span class="strut bottom" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="base textstyle uncramped"><span class="mord mathrm">3</span><span class="mord mathrm">0</span><span class="mbin">+</span><span class="mord mathrm">9</span><span class="mord mathrm">0</span><span class="mrel">=</span><span class="mord mathrm">1</span><span class="mord mathrm">2</span><span class="mord mathrm">0</span></span></span></span></p>
<p>Then for each neuron we want to know what percentage of this sum it represent:</p>
<p><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>3</mn><mn>0</mn><mi mathvariant="normal">/</mi><mn>1</mn><mn>2</mn><mn>0</mn><mo>=</mo><mn>0</mn><mi mathvariant="normal">.</mi><mn>2</mn><mn>5</mn></mrow><annotation encoding="application/x-tex">30 / 120 = 0.25</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord mathrm">3</span><span class="mord mathrm">0</span><span class="mord mathrm">/</span><span class="mord mathrm">1</span><span class="mord mathrm">2</span><span class="mord mathrm">0</span><span class="mrel">=</span><span class="mord mathrm">0</span><span class="mord mathrm">.</span><span class="mord mathrm">2</span><span class="mord mathrm">5</span></span></span></span><br>
<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>9</mn><mn>0</mn><mi mathvariant="normal">/</mi><mn>1</mn><mn>2</mn><mn>0</mn><mo>=</mo><mn>0</mn><mi mathvariant="normal">.</mi><mn>7</mn><mn>5</mn></mrow><annotation encoding="application/x-tex">90 / 120 = 0.75</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord mathrm">9</span><span class="mord mathrm">0</span><span class="mord mathrm">/</span><span class="mord mathrm">1</span><span class="mord mathrm">2</span><span class="mord mathrm">0</span><span class="mrel">=</span><span class="mord mathrm">0</span><span class="mord mathrm">.</span><span class="mord mathrm">7</span><span class="mord mathrm">5</span></span></span></span></p>
<p>Now we are sure each neuron output a percentage between 0 and 1 and that they
all sum to 1.</p>
<p>For those interested softmax add just a little trick to the above: it replace
all values with an exponential so that bigger values get even bigger confidences
and lower values get even lower confidences.</p>
<p>TODO: softmax equation when frac will works...</p>
<p>So why did we bother doing all those changes while we could have done the same
thing with our previous one neuron method ? Well, now we can add as many classes
as we want and it will predict which of these classes is the most probable.</p>
<p>TODO: image</p>
<p>It should be useful for our project ;)</p>
<p>For this kind of problem, the labels are passed as one hot vectors. One hot
vectors are arrays filled with 0 except for one value that will be 1. Let's say
we want to detect what animal is represented in an image. We have 3 possible
classes: bird, cat and dog.</p>
<ul>
<li>If the image represent a bird, the one hot vector will be [1, 0, 0]</li>
<li>If the image represent a cat,  the one hot vector will be [0, 1, 0]</li>
<li>If the image represent a dog,  the one hot vector will be [0, 0, 1]</li>
</ul>
<p>If you have thousands of classes, it can become annoying to create those array.
Instead we would prefer using a number to represent the class represented in the
image.</p>
<ul>
<li>If the image represent a bird, the label is 0</li>
<li>If the image represent a cat,  the label is 1</li>
<li>If the image represent a dog,  the label is 2</li>
</ul>
<p>And fortunately keras allows us to specify labels as number and will convert
them as one hot vectors by using <code>sparse_categorical_crossentropy</code>.</p>
<h1>classification example (keep or not ? simplify ?)</h1>
<p>We've seen how to solve many different problems and one nice thing is that we
can mix those methods. Let's build a network for the following task: we have an
image and we want to know if there is an animal in it.
It's a classic binary classification problem, so we can use one neuron with a
sigmoid activation and a binary_crossentropy loss. If the output is &gt; 0.5 then
there is an animal otherwise, there is not.
Now if there is an animal we also want to predict its position in the image.
So we keep the previous network and we add two new neuron, one for predicting x
and one for predicting y. Both can return any value so we don't use any
activation and we use a mean squared error loss as this is a regression problem.
Then we also want to know which kind of animal it is.
So once again we reuse the previous network and add more neurons: one per kind
of animal. These neurons will then be followed by a softmax activation layer and
will use a categorical_crossentropy loss.</p>
<p>And that's it, we built a network that can perform all those tasks from small
buildings blocks. As long you have the desired labels, you can train your
network to perform these tasks.</p>
<p>While building these more complex networks is pretty straightforward with
libraries like keras, it would require us to learn a few more things about
keras' api that i don't want to cover in this post. But take a look at keras'
functional API if you want to know.</p>
<div class="note">
<p>One may think that by giving many tasks to a single network it will perform
worse. But in fact, it's the opposite: by giving a network many related tasks
like we did above, will help the network to produce better predictions.</p>
</div>
<h1>Mnist</h1>
<p>Until now we always trained our network will all the inputs it will ever need to
make a prediction for. Thus we had 100% correct predictions. But in the real
world, this is not something feasable. For our digits recognition task, there
exist an infinity of possible images. What's matter is to train our network with
a variety of different images so that it will still be able to make prediction
for images it has never seen.</p>
<p>So our first task is to get a dataset of labelled images. There exists plenty of
free datasets available online and some of the most famous are directly
integrated into keras. One of them, the mnist dataset, contains labelled
pictures of hand written digits.</p>
<pre><code class="language-python"><span class="hljs-keyword">from</span> keras.datasets <span class="hljs-keyword">import</span> mnist
</code></pre>
<p>We then load the inputs and labels. They are splits in two datasets:</p>
<ul>
<li>train: the one we use to train our network</li>
<li>test : the one we use to see how well our network perform with new data</li>
</ul>
<pre><code class="language-python">(x_train, y_train), (x_test, y_test) = mnist.load_data()
</code></pre>
<p>Let's play with our input data:</p>
<pre><code class="language-python">x_train[<span class="hljs-number">0</span>].shape

<span class="hljs-comment">#&gt; (28, 28)</span>
</code></pre>
<p>We can see that our images are represented by 2d arrays. Let's look at the first
one:</p>
<pre><code class="language-python">x_train[<span class="hljs-number">0</span>]
<span class="hljs-comment">#&gt; array([[  0, 0, 0, 0, 0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 0, 0, 0],</span>
<span class="hljs-comment">#&gt;        [  0, 0, 0, 0, 0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 0, 0, 0],</span>
<span class="hljs-comment">#&gt;        [  0, 0, 0, 0, 0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 0, 0, 0],</span>
<span class="hljs-comment">#&gt;        [  0, 0, 0, 0, 0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 0, 0, 0],</span>
<span class="hljs-comment">#&gt;        [  0, 0, 0, 0, 0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 0, 0, 0],</span>
<span class="hljs-comment">#&gt;        [  0, 0, 0, 0, 0,   0,   0,   0,   0,   0,   0,   0,   3,   18,  18,  18,  126, 136, 175, 26,  166, 255, 247, 127, 0, 0, 0, 0],</span>
<span class="hljs-comment">#&gt;        [  0, 0, 0, 0, 0,   0,   0,   0,   30,  36,  94,  154, 170, 253, 253, 253, 253, 253, 225, 172, 253, 242, 195, 64,  0, 0, 0, 0],</span>
<span class="hljs-comment">#&gt;        [  0, 0, 0, 0, 0,   0,   0,   49,  238, 253, 253, 253, 253, 253, 253, 253, 253, 251, 93,  82,  82,  56,  39,  0,   0, 0, 0, 0],</span>
<span class="hljs-comment">#&gt;        [  0, 0, 0, 0, 0,   0,   0,   18,  219, 253, 253, 253, 253, 253, 198, 182, 247, 241, 0,   0,   0,   0,   0,   0,   0, 0, 0, 0],</span>
<span class="hljs-comment">#&gt;        [  0, 0, 0, 0, 0,   0,   0,   0,   80,  156, 107, 253, 253, 205, 11,  0,   43,  154, 0,   0,   0,   0,   0,   0,   0, 0, 0, 0],</span>
<span class="hljs-comment">#&gt;        [  0, 0, 0, 0, 0,   0,   0,   0,   0,   14,  1,   154, 253, 90,  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 0, 0, 0],</span>
<span class="hljs-comment">#&gt;        [  0, 0, 0, 0, 0,   0,   0,   0,   0,   0,   0,   139, 253, 190, 2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 0, 0, 0],</span>
<span class="hljs-comment">#&gt;        [  0, 0, 0, 0, 0,   0,   0,   0,   0,   0,   0,   11,  190, 253, 70,  0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 0, 0, 0],</span>
<span class="hljs-comment">#&gt;        [  0, 0, 0, 0, 0,   0,   0,   0,   0,   0,   0,   0,   35,  241, 225, 160, 108, 1,   0,   0,   0,   0,   0,   0,   0, 0, 0, 0],</span>
<span class="hljs-comment">#&gt;        [  0, 0, 0, 0, 0,   0,   0,   0,   0,   0,   0,   0,   0,   81,  240, 253, 253, 119, 25,  0,   0,   0,   0,   0,   0, 0, 0, 0],</span>
<span class="hljs-comment">#&gt;        [  0, 0, 0, 0, 0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   45,  186, 253, 253, 150, 27,  0,   0,   0,   0,   0, 0, 0, 0],</span>
<span class="hljs-comment">#&gt;        [  0, 0, 0, 0, 0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   16,  93,  252, 253, 187, 0,   0,   0,   0,   0, 0, 0, 0],</span>
<span class="hljs-comment">#&gt;        [  0, 0, 0, 0, 0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   249, 253, 249, 64,  0,   0,   0,   0, 0, 0, 0],</span>
<span class="hljs-comment">#&gt;        [  0, 0, 0, 0, 0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   46,  130, 183, 253, 253, 207, 2,   0,   0,   0,   0, 0, 0, 0],</span>
<span class="hljs-comment">#&gt;        [  0, 0, 0, 0, 0,   0,   0,   0,   0,   0,   0,   0,   39,  148, 229, 253, 253, 253, 250, 182, 0,   0,   0,   0,   0, 0, 0, 0],</span>
<span class="hljs-comment">#&gt;        [  0, 0, 0, 0, 0,   0,   0,   0,   0,   0,   24,  114, 221, 253, 253, 253, 253, 201, 78,  0,   0,   0,   0,   0,   0, 0, 0, 0],</span>
<span class="hljs-comment">#&gt;        [  0, 0, 0, 0, 0,   0,   0,   0,   23,  66,  213, 253, 253, 253, 253, 198, 81,  2,   0,   0,   0,   0,   0,   0,   0, 0, 0, 0],</span>
<span class="hljs-comment">#&gt;        [  0, 0, 0, 0, 0,   0,   18,  171, 219, 253, 253, 253, 253, 195, 80,  9,   0,   0,   0,   0,   0,   0,   0,   0,   0, 0, 0, 0],</span>
<span class="hljs-comment">#&gt;        [  0, 0, 0, 0, 55,  172, 226, 253, 253, 253, 253, 244, 133, 11,  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 0, 0, 0],</span>
<span class="hljs-comment">#&gt;        [  0, 0, 0, 0, 136, 253, 253, 253, 212, 135, 132, 16,  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 0, 0, 0],</span>
<span class="hljs-comment">#&gt;        [  0, 0, 0, 0, 0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 0, 0, 0],</span>
<span class="hljs-comment">#&gt;        [  0, 0, 0, 0, 0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 0, 0, 0],</span>
<span class="hljs-comment">#&gt;        [  0, 0, 0, 0, 0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 0, 0, 0]], dtype=uint8)</span>
</code></pre>
<p>While we were using only black and white pixels, here we have greyscale images.
So pixels value range from 0 to 255, 0 being black, 255 being white and all the
values in between being shades of grey.</p>
<p>We can guess from the pattern that the image represent a 5, but let's display
the image to confirm:</p>
<pre><code class="language-python">imshow(x_train[<span class="hljs-number">0</span>])

<span class="hljs-comment">#&gt; insert in notebook &amp; blog ?</span>
</code></pre>
<p>Looks like our prediction was right. But we could also have used the labels
associated with this image:</p>
<pre><code class="language-python">y_train[<span class="hljs-number">0</span>]

<span class="hljs-comment">#&gt; 5</span>
</code></pre>
<p>So we have inputs and we have labels. Let's build our network !</p>
<p>Currently we only learned how to create network with 1d inputs, so we need to
reshape the input from 28x28 to an array of 784 elements:</p>
<pre><code class="language-python">x_test  = x_test .reshape((x_test .shape[<span class="hljs-number">0</span>], <span class="hljs-number">-1</span>))
x_train = x_train.reshape((x_train.shape[<span class="hljs-number">0</span>], <span class="hljs-number">-1</span>))

x_train[<span class="hljs-number">0</span>].shape

<span class="hljs-comment">#&gt; (784,)</span>
</code></pre>
<p>Now we can pass those array as inputs.</p>
<p>Let's begin with a simple network with a single hidden layer.
We said that hidden layers should have an activation and that ReLu was a good
default. We also said that Adam was a good optimizer, so let's use them.
As for the output, we have 10 classes, one for each digit, from 0 to 9. And we
want to predict which class is the most likely. So we will use 10 neurons, with
a softmax activation and a categorical_crossentropy loss.
But we've seens that the labels are represented by numbers and we know that the
network only accept one hot vectors. So we ask keras to convert those numbers to
one hot vectors by using sparse_categorical_crossentropy.</p>
<p>Now let's pick an arbitrary number of neurons and train the network for a few
steps.</p>
<p>TODO: image of current network</p>
<pre><code class="language-python"><span class="hljs-keyword">from</span> keras.models <span class="hljs-keyword">import</span> Sequential
<span class="hljs-keyword">from</span> keras.layers <span class="hljs-keyword">import</span> Dense

model = Sequential()
model.add(Dense(<span class="hljs-number">32</span>, activation = <span class="hljs-string">"relu"</span>, input_shape = x_train[<span class="hljs-number">0</span>].shape))
model.add(Dense(<span class="hljs-number">10</span>, activation = <span class="hljs-string">"softmax"</span>))

model.compile(optimizer = <span class="hljs-string">"adam"</span>, loss = <span class="hljs-string">"sparse_categorical_crossentropy"</span>, metrics = [<span class="hljs-string">"accuracy"</span>])
model.fit(x_train, y_train, epochs = <span class="hljs-number">3</span>)
</code></pre>
<p>TODO: image predict &amp; plot</p>
<p>Run the last line until the accuracy stall. It stopped close to 0.6 on my
computer (don't worry if you don't find the same results and keep going). That
means that out of 10 pictures, 6 are correctly classified. Not bad for 5 lines
of code and a few seconds of computation. But this accuracy is not really useful
because the network has been trained on those images. It would be better to see
how it performs on unseen images. Let's use the test dataset we kept for this
exact reason.</p>
<pre><code class="language-python">model.evaluate(x_test, y_test)

<span class="hljs-comment">#&gt;</span>
</code></pre>
<p>The first number is the value of the loss and the second is the accuracy. You
should get about the same accuracy you got on the training dataset. Seems like
our network perform the same on both datasets.</p>
<p>As we said, we have to try some different architectures and keep the one that
work best. So let's try with more neurons:</p>
<pre><code class="language-python">model = Sequential()
model.add(Dense(<span class="hljs-number">64</span>, activation = <span class="hljs-string">"relu"</span>, input_shape = x_train[<span class="hljs-number">0</span>].shape))
model.add(Dense(<span class="hljs-number">10</span>, activation = <span class="hljs-string">"softmax"</span>))

model.compile(optimizer = <span class="hljs-string">"adam"</span>, loss = <span class="hljs-string">"sparse_categorical_crossentropy"</span>, metrics = [<span class="hljs-string">"accuracy"</span>])
model.fit(x_train, y_train, epochs = <span class="hljs-number">3</span>)

model.evaluate(x_test, y_test)
</code></pre>
<p>After running for 6 epochs, I got a better score of 0.7 for both datasets: 7
images out of 10 are correctly classified.</p>
<p>Let's try with a deeper network:</p>
<pre><code class="language-python">model = Sequential()
model.add(Dense(<span class="hljs-number">32</span>, activation = <span class="hljs-string">"relu"</span>, input_shape = x_train[<span class="hljs-number">0</span>].shape))
model.add(Dense(<span class="hljs-number">32</span>, activation = <span class="hljs-string">"relu"</span>))
model.add(Dense(<span class="hljs-number">32</span>, activation = <span class="hljs-string">"relu"</span>))
model.add(Dense(<span class="hljs-number">10</span>, activation = <span class="hljs-string">"softmax"</span>))

model.compile(optimizer = <span class="hljs-string">"adam"</span>, loss = <span class="hljs-string">"sparse_categorical_crossentropy"</span>, metrics = [<span class="hljs-string">"accuracy"</span>])
model.fit(x_train, y_train, epochs = <span class="hljs-number">6</span>)

model.evaluate(x_test, y_test)
</code></pre>
<p>Now after 6 epochs, I get an accuracy greater than 0.9 on both datasets. Do not
forget to evaluate your network on your test dataset otherwise you could end up
predicting the good value 100% of the time for images you've trained on but
perform poorly on unseen images. This is called overfitting.</p>
<p>Now let's say that this accuracy suits us, we can use this network to make
predictions with <code>predict_classes()</code>:</p>
<pre><code class="language-python">model.predict_classes(x_test[:<span class="hljs-number">10</span>])
</code></pre>
<p>If we spent more time playing with the network we could get an even better
accuracy but it's not our objective here.</p>
<p>Now let's take a step back: why did we do all this ? Couldn't we come up with a
program that perform better and gives us the the good answer each time ? Well...
no. Recognizing a digit may seems a simple task, but imagine all the positions
and deformations a digit can undergo. There would be too many cases your program
would fail to handle. Instead we let the networks figure out those rules for us.
And it does a better job at finding those rules than programs built by humans
experts.
It's also worth noting that the rules the network learn are often different from
what a human would come with. Thus we often don't understand what our networks
are doing.</p>
<p>I'd would also like to point out that we may think that a human would have 100%
accuracy on this kind of task, but that's not true. Take thoses pictures for
example:</p>
<p>TODO: images</p>
<p>It was ''. many people would get them wrong ! So there is always some small
margin for computers to improve upon human. And in fact for single character
recognition, machines are already better than humans since a few years =)</p>
<p>And this is the end of our first machine learning project.</p>
<hr>
<p>This post has been long, and there is so much things left to cover.</p>
<p>TODO: ... (move to another file)</p>
<p>We also only looked at the bright side of neural networks. However they have
numerous pitfalls that we didn't talk about.</p>
<p>I hope this post helped you !</p>
<div class="note">
<p>Feedbacks, whether good or bad, are more than welcome ! I would love to hear
about your suggestions to make the content more accessible and understandable =)</p>
</div>
<p>What to expect:</p>
<ul>
<li>better english (better sentences &amp; spelling)</li>
<li>3d examples</li>
<li>intro examples &amp; all the todos :P</li>
<li>titles &amp; toc</li>
<li>improved tools to generate images/animations and articles (should take a few
weeks but will allow me to produce content much more faster)</li>
<li>js compatibility, improved load time, css &amp; jekyll</li>
<li>explain what's next &amp; publish introductory post</li>
<li>and lots of other stuff</li>
</ul>

		</div>
	</body>
	</html>
	