
	<!DOCTYPE html>
	<html>
	<head>
		<meta charset="UTF-8">

		<!--
		<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/meyer-reset/2.0/reset.min.css" />
		<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/normalize/7.0.0/normalize.min.css" />
		-->

		<link rel="stylesheet" href="/node_sources/part-x/index-v1.css">
		<link rel="stylesheet" href="/node_modules/katex/dist/katex.min.css">
		<link rel="stylesheet" href="/node_modules/highlight.js/styles/default.css">

		<link rel="stylesheet" href="/node_modules/slick-carousel/slick/slick.css">
		<link rel="stylesheet" href="/node_modules/slick-carousel/slick/slick-theme.css">

		<script src="/node_modules/lodash/lodash.min.js"></script>
		<script src="/node_modules/highcharts/highcharts.js"></script>
		<script src="/node_modules/jquery/dist/jquery.min.js"></script>
		<script src="/node_modules/slick-carousel/slick/slick.min.js"></script>

		<link rel="stylesheet" href="//code.jquery.com/ui/1.12.1/themes/base/jquery-ui.css">
		<script src="https://code.jquery.com/ui/1.12.1/jquery-ui.js"></script>
	</head>
	<body>
		<div id="content">
			<h1>Detecting one white pixel</h1>
<p>Our first model will be a white pixel detector: given a pixel, we want to
classify it as white or not white. In machine learning we often represent 'yes'
as 1 and 'no' as 0.</p>
<p><img src="/node_outputs/part-1/images-js/logic-id-model.png" alt=""></p>
<p>In order to do that, we will use what's called a neural network. If you've never
seen one before, it's something that look like that:</p>
<p><img src="/node_outputs/part-1/images-js/neural-net-2332.png" alt=""></p>
<p>Those networks are made of small pieces called neurons.</p>
<p><img src="/node_outputs/part-1/images-js/neuron-neuron.png" alt=""></p>
<p>A neuron has one or more inputs. In our case, the input is only one pixel value.</p>
<p><img src="/node_outputs/part-1/images-js/neuron-input.png" alt=""></p>
<p>Each input is linked to the neuron by a weight</p>
<p><img src="/node_outputs/part-1/images-js/neuron-weight.png" alt=""></p>
<p>Remember that in our examples, there is only two pixel colors: white and black.
So there is only two possible inputs:</p>
<p><img src="/node_outputs/part-1/images-js/nnet-id-inputs.png" alt=""></p>
<p>Let's add the desired output in each case:</p>
<p><img src="/node_outputs/part-1/images-js/nnet-id-outputs.png" alt=""></p>
<p>And that's all we need to know to solve this problem ! Don't worry about the
brown neurons, we will cover them later when we will see bigger networks.</p>
<h2>Code</h2>
<p>In this post and those who follow, we will be using keras, a high level neural
network python library. It will allow us to focus on how things work instead
of worying about implementation details. You can install it using <code>pip install keras</code>.</p>
<p>First, we import the required modules and declare some useful variables.</p>
<pre><code class="language-python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np

<span class="hljs-keyword">from</span> keras.layers <span class="hljs-keyword">import</span> Dense
<span class="hljs-keyword">from</span> keras.models <span class="hljs-keyword">import</span> Sequential

BLACK = <span class="hljs-number">0</span>
WHITE = <span class="hljs-number">1</span>

<span class="hljs-comment">#&gt; Using TensorFlow backend.</span>
</code></pre>
<p>Then we create our model and our neuron.</p>
<pre><code class="language-python"><span class="hljs-comment"># create a new model</span>
pixel_is_white_model = Sequential()
<span class="hljs-comment"># create a single neuron with a single input and add it to our model</span>
pixel_is_white_model.add(Dense(<span class="hljs-number">1</span>, use_bias = <span class="hljs-keyword">False</span>, input_shape = (<span class="hljs-number">1</span>,)))
</code></pre>
<p>As we can see, a neuron is created with <code>Dense</code>. The first argument is the
number of neurons we want to create and <code>input_shape</code> is the number of inputs.<br>
Don't worry about <code>Sequential</code> and <code>use_bias</code>, we will talk about them later.</p>
<p>Now that our model is ready, we need to set up our training. As described in the
introduction, we will use a supervised learning technique: we will show our
model different pixel values and correct/reward it depending on its answers.
To do that we need to prepare a dataset comprised of pixel values and their
expected outputs (called labels).</p>
<pre><code class="language-python">pixel_is_white_inputs = [BLACK, WHITE]
pixel_is_white_labels = [<span class="hljs-number">0</span>,     <span class="hljs-number">1</span>]
</code></pre>
<p>Then we can train our model.</p>
<pre><code class="language-python"><span class="hljs-comment"># disregard this function for now, it will be explained later</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">train</span><span class="hljs-params">(model, x, y)</span>:</span>
    model.compile(
        optimizer = <span class="hljs-string">"sgd"</span>,
        loss      = <span class="hljs-string">"mean_squared_error"</span>,
        metrics   = [<span class="hljs-string">"accuracy"</span>]
    )
    model.fit(x, y, epochs = <span class="hljs-number">1000</span>)

<span class="hljs-comment"># show our pixels to our model and correct/reward it</span>
train(
    pixel_is_white_model,
    pixel_is_white_inputs,
    pixel_is_white_labels,
)

<span class="hljs-comment">#&gt; ...</span>
<span class="hljs-comment">#&gt; 2/2 [==============================] - 0s - loss: 1.0960e-07 - acc: 1.0000</span>
<span class="hljs-comment">#&gt; Epoch 1999/2000</span>
<span class="hljs-comment">#&gt; 2/2 [==============================] - 0s - loss: 1.0960e-07 - acc: 1.0000</span>
<span class="hljs-comment">#&gt; Epoch 2000/2000</span>
<span class="hljs-comment">#&gt; 2/2 [==============================] - 0s - loss: 1.0960e-07 - acc: 1.0000</span>
</code></pre>
<div class="note">
<p>Don't pay too much attention about the output. We just want to make sure that
'acc', short for accuracy, is equal to 1.0000 (i.e our model predict the good
value 100% of the time).<br>
If not, run <code>train()</code> until you get this result.</p>
</div>
<p>And that's it ! We have a trained model ready to be used ! Let's use the
<code>predict_classes</code> method to get our model prediction. For each input, it returns
the predicted class: 1 for white and 0 otherwise.</p>
<pre><code class="language-python">pixel_is_white_model.predict_classes(np.array([BLACK]))

<span class="hljs-comment">#&gt; array([[0]], dtype=int32)</span>

pixel_is_white_model.predict_classes(np.array([WHITE]))

<span class="hljs-comment">#&gt; array([[1]], dtype=int32)</span>

pixel_is_white_model.predict_classes(pixel_is_white_inputs)

<span class="hljs-comment">#&gt; array([[0],</span>
<span class="hljs-comment">#&gt;        [1]], dtype=int32)</span>
</code></pre>
<p>Nice, it predicted exactly what we wanted ! And you may not believe it, but this
will be almost the whole code we need for handwritten digits recognition !</p>
<h1>Intuition and experiments</h1>
<p>How does this compute anything at all ? Here is the trick: each weight has a
value.</p>
<p><img src="/node_outputs/part-1/images-js/nnet-id-weight.png" alt=""></p>
<p>A neuron value is equal to the input times the weight:</p>
<p><img src="/node_outputs/part-1/images-js/nnet-id-equation.png" alt=""></p>
<p>If we input a white pixel, the equation become:</p>
<p><img src="/node_outputs/part-1/images-js/nnet-id-input-white.png" alt=""></p>
<p>And we know that for a white pixel, the desired output is 1.</p>
<p><img src="/node_outputs/part-1/images-js/nnet-id-output-white.png" alt=""></p>
<p>So we need to find <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>w</mi></mrow><annotation encoding="application/x-tex">w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.02691em;">w</span></span></span></span>, the weight value, that solve this equation. But
remember, we don't have only one possible input: the weight must work for each
case.</p>
<p><img src="/node_outputs/part-1/images-js/nnet-id-equations.png" alt=""></p>
<p>Here, we can find the solution by ourselves: <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>w</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">w = 1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.64444em;"></span><span class="strut bottom" style="height:0.64444em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.02691em;">w</span><span class="mrel">=</span><span class="mord mathrm">1</span></span></span></span></p>
<p><img src="/node_outputs/part-1/images-js/nnet-id-solutions.png" alt=""></p>
<p>And this is exactly what the library is doing for us ! No matter how complex
your network is, training is just a matter of finding good weights.</p>
<p>Let's experiment with what we learned. Here is an equivalent of our current
network in plain python:</p>
<pre><code class="language-python">weight = <span class="hljs-number">1</span>

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">neuron</span><span class="hljs-params">(input_pixel)</span>:</span>
    <span class="hljs-keyword">return</span> input_pixel * weight

neuron(BLACK)

<span class="hljs-comment">#&gt; 0</span>

neuron(WHITE)

<span class="hljs-comment">#&gt; 1</span>
</code></pre>
<p>As we will see later, it's almost impossible for our library to find exactly the
best weight's value. Even in this simple case it did not find exactly 1:</p>
<pre><code class="language-python"><span class="hljs-comment"># do not think too much about this line, it just returns the model's weight</span>
weight_model = pixel_is_white_model.get_weights()[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>][<span class="hljs-number">0</span>]
weight_model

<span class="hljs-comment">#&gt; 0.99861383</span>
</code></pre>
<p>If we wanted to get the real output, we should have used the <code>predict</code> method.</p>
<pre><code class="language-python">pixel_is_white_model.predict(np.array([BLACK]))

<span class="hljs-comment">#&gt; array([[ 0.]], dtype=float32)</span>

pixel_is_white_model.predict(np.array([WHITE]))

<span class="hljs-comment">#&gt; array([[ 0.99861383]], dtype=float32)</span>
</code></pre>
<p>We can now update our previous implementation to use the model's weight:</p>
<pre><code class="language-python">weight = weight_model

neuron(BLACK)

<span class="hljs-comment">#&gt; 0.0</span>

neuron(WHITE)

<span class="hljs-comment">#&gt; 0.99861383438110352</span>
</code></pre>
<p>And we get the exact same results as our network !</p>
<p>For those who are curious, <code>predict_classes</code> is equivalent to (for one output
only):</p>
<pre><code class="language-python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">predict_classes</span><span class="hljs-params">(output)</span>:</span>
    <span class="hljs-keyword">if</span> output &gt; <span class="hljs-number">0.5</span>:
        <span class="hljs-keyword">return</span> <span class="hljs-number">1</span>
    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>

predict_classes(neuron(WHITE))

<span class="hljs-comment">#&gt; 1</span>
</code></pre>

		</div>
	</body>
	</html>
	